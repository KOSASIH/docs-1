---
title: "Logging"
metaTitle: "DeepSparse Logging"
metaDescription: "System and Data Logging with DeepSparse"
index: 6000
---

# DeepSparse Logging

DeepSparse Logging provides you with access to telemetry needed to monitor a deployment. 

There are many types of downstream monitoring tasks that you may want to perform. The difficulty of the tasks varies from relatively easy 
(simple system performance analysis) to challenging (assessing the accuracy of the system in the wild by manually labeling 
the input data distribution post-factum). Examples include:
- System performance: what is the latency/throughput of a query?
- Data quality: is there an issue getting data to my model?
- Data distribution shift: does the input data distribution deviates over time to the point where the model stops to deliver reliable predictions?
- Model accuracy: what is the percentage of correct predictions that a model achieves?

DeepSparse Logging is designed to provide maximum flexibility for you to extract whatever data is needed from a 
production inference pipeline into the logging system of your choice. 

## Metrics 
DeepSparse Logging provides access to two types of metrics.

### System Logging Metrics 

System Logging Metrics give you access to granular performance metrics for quick and efficient diagnosis of deployment system health.

There is one group of System Logging Metrics currently available: Inference Latency. For each inference request, the DeepSparse Server logs the following:
1. Pre-processing Time - milliseconds in the pre-processing step
2. Engine Time - milliseconds in the engine forward pass step
3. Post-processing Time - milliseconds in the post-processing step
4. Total Time - milliseconds for the end-to-end response time

### Data Logging Metrics

Data Logging Metrics give ML teams access to data at each stage of an inference pipeline. 
This facilitates inspection of the data, understanding of its properties, detecting edge cases and possible data drift.

There are 4 stages in the inference pipeline where Data Logging can occur:
1. `pipeline_inputs`: raw input passed to the inference pipeline by the user
2. `engine_inputs`: pre-processed tensors passed to the engine for the forward pass
3. `engine_outputs`: result of the engine forward pass (e.g. the raw logits)
4. `pipeline_outputs`: raw output returned to the pipeline caller

At each stage, you can specify functions to be applied to the data before Logging. Examples functions include the identity function
(for logging the raw input/output), the mean function (e.g. for monitoring the mean pixel value of an image), or a count function (e.g.
for monitoring the number of unknown tokens in an NLP pipeline).

There are three types of functions that can be applied to target data at each stage:
1. Built-in functions - pre-written functions provided by DeepSparse ([see list on GitHub](https://github.com/neuralmagic/deepsparse/blob/main/src/deepsparse/loggers/metric_functions/built_ins.py)) 
2. Framework Functions - functions from `torch` or `numpy` (e.g. `torch.mean`, `np.mean`)
3. Custom Functions - custom user-provided functions with dynamic imports

## Configuration

A YAML file is used to configure both System and Data Logging.
- **System Logging** is *enabled* if a `logger` is specified
- **Data Logging** is *disabled* by default. The config allows you to specify what data to log

The YAML file has the following form:

```yaml
loggers:
  logger_name1:
  logger_name2:
    arg1: 6100                    
    
data_logging:
  # either specify the pipeline stage ... 
  stage:
    - func: built_in_fn_name
      frequency: 1000
      target_loggers:
        - logger_name1
  
  # ... or a property of the data at a pipeline stage ...
  stage.target:
    - func: torch.fn_name
      frequency: 1000
      target_loggers:
        - logger_name2

  # ... or a slice/dict access to a target
  stage.target[idx]:
    - func: np.fn_name        
      frequency: 1000          
    - func: path/to/custom.py:bar  
      frequency: 10000
      # if no target_loggers, defaults to all
```

Let's break it down piece by piece:

- `loggers` is a dictionary of the form `{logger_name: {config}}` and identifies and provides the arguments for each logger.

- `data_logging` is a dictionary of the form `{stage.target[idx]: [fn_config]}`. 
For each item, you can provide a list of function configs tha specify which function to apply.

- `stage.target[idx]` specifies if data at a particular `stage` should be logged. If the data at a particular `stage` has
properties, an optional `target` can be used to unpack the data. If the `target` is a dict or a list, 
you can access via slicing, indexing, or dict access, for example `stage.target[0:2][0, :-1]['key_3'][5]`.

- `[fn_config]` is list of function configurations of the form `{func: name, frequency: freq, target_loggers: [logger_names]}`. 
It specifies which the function, the logger, and the frequency of the logging.

<details>
  <summary>Click for a tangible example for an image classification endpoint.</summary>

```yaml
loggers:
  python:         # logs to stdout
  prometheus:     # logs to prometheus on port 6100
    port: 6100

endpoints:
  - task: image_classification
    route: /image_classification/predict
    model: zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none
    name: image_classification_pipeline
    data_logging:
      pipeline_inputs.images:   # applies to the images (of the form stage.target:)
        - func: np.shape        # framework function
          frequency: 1
          target_loggers:
            - python          

      pipeline_inputs.images[0]:          # applies to the first image (of the form stage.target[idx]:)
        - func: mean_pixels_per_channel   # built-in function
          frequency: 2
          target_loggers:
            - prometheus        
        - func: fraction_zeros  # built-in function
          frequency: 2
          target_loggers:
            - prometheus
      
      engine_inputs:            # applies to the engine_inputs data (of the form stage:)
        - func: identity        # built-in function
          frequency: 1
          target_loggers:
            - python
        - func: np.shape        # framework function
          frequency: 1
          target_loggers:
            - python
```

This configuration does the following data logging at each respective stage of the pipeline:
- System logging is enabled by default and logs to Prometheus
- Logs the shape of the input batch provided by the user to stdout
- Logs the mean pixels and % of 0 pixels of the first image in the batch to Prometheus
- Logs the raw data and shape of the input passed to the engine
- No logging occurs at any other pipeline stages

</details>

## Loggers

DeepSparse Logging includes options to log to Standard Output and to Prometheus out of the box as well as 
the ability to create a Custom Logger.

### Python Logger

Python Logger logs data to Standard Output. It is useful for debugging and inspecting an inference pipeline. It
accepts no arguments and is configured with the following:

```yaml
loggers:
  python:
```

### Prometheus Logger

DeepSparse is integrated with Prometheus. The Prometheus Logger accepts an optional `port` property 
which specifies where the metrics will be exposed:

```yaml
loggers:
  prometheus:
    port: 6100  # < optional, if not provided defauls to 6100
```

There are four types of metrics in Prometheus (Counter, Gauge, Summary, and Histogram). Under the hood, DeepSparse uses the 
[Summary metric](https://prometheus.io/docs/concepts/metric_types/#summary), so make sure the data you are logging is 
compatible with this metric type.

#### Additional Resources
Checkout our example using [DeepSparse with Prometheus / Grafana](https://github.com/neuralmagic/deepsparse/tree/rs-logging-sdk/logging-sdk/tutorial-server-prometheus)

### Custom Logger

If you need a custom logger (for example, to log high dimensional data like raw image inputs), 
you can create a class that inherits from the `BaseLogger` and implements the `log` method. 
The `log` method is called at each Pipeline stage (`pipeline_inputs`, `engine_inputs`, `engine_outputs`, `pipline_outputs`) 
and should handle exposing the metric to the Logger. 

```python
from deepsparse.logging import BaseLogger, MetricsCategories
from typing import Any, Optional

class CustomLogger(BaseLogger):
    def log(self, identifier: str, value: Any, category: Optional[str]=None):
        """
        :param identifier: The name of the item that is being logged.
            By default, in the simplest case, that would be a string in the form
            of "<pipeline_name>/<logging_target>"
            e.g. "image_classification/pipeline_inputs"
        :param value: The item that is logged along with the identifier
        :param category: The metric category that the log belongs to. 
            By default, we recommend sticking to our internal convention
            established in the MetricsCategories enum.
        """
```

Once a custom logger is implemented, it can be referenced from a config file:

```yaml
loggers:
  custom_logger:
    path: path/to/customer-logger-script.py:CustomLogger
    arg1: your_argument_1
```

See [our Prometheus logger implementation](https://github.com/neuralmagic/deepsparse/blob/main/src/deepsparse/loggers/prometheus_logger.py)
for inspiration on implementing a logger.

## Usage 

DeepSparse Logging is currently supported for usage with DeepSparse Server. 

Stay tuned for usage with Pipelines in our next release!

### Server Usage

The Server startup CLI command accepts a YAML configuration file (which contains both logging-specific and general 
configuration details) via the `--config-file` argument.

```bash
deepsparse.server --config-file server-config.yaml
```

Data Logging is configured at the endpoint level. In the example below, we create a Server with two Endpoints
(one for image classification and one for sentiment analysis):

```yaml
loggers:
  prometheus:
    port: 6100
  python:
 
endpoints:
  - task: image_classification
    route: /image_classification/predict
    model: zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none
    name: image_classification_pipeline
    data_logging:
      pipeline_inputs.images:
        - func:
          frequency: 1
          target_loggers:
            - python

      pipeline_inputs.images[0]
        - func: max_pixels_per_channel
          frequency: 1
          target_loggers:
            - prometheus
        - func: mean_pixels_per_channel
          frequency: 1
          target_loggers:
            - prometheus
        - func: fraction_zeros
          frequency: 1
          target_loggers:
            - prometheus
      
      pipeline_outputs.scores[0]:
        - func: identity
          frequency: 1
      
      pipeline_outputs.labels[0]:
        - func: identity
          frequency: 1

  - task: sentiment/analysis
    route: /sentiment_analysis/predict
    model: zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer-pruned80_quant-none-vnni
    name: sentiment_analysis_pipeline
    data_logging:
      engine_inputs:
        - func: path/to/example_custom_fn.py:sequence_length
          frequency: 1
          
      pipeline_outputs.scores[0]:
        - func: identity
          frequency: 1
```

The `example_custom_fn.py` file could look like the following:

```python
import numpy as np
from typing import List

# Engine inputs to transformers is 3 lists of np.arrays representing
# the encoded input, the attention mask, and token types.
# Each of the np.arrays is of shape (batch, max_seq_len), so
# engine_inputs[0][0] gives the encodings of the first item in the batch.
# The number of non-zeros in this slice is the sequence length.
def sequence_length(engine_inputs: List[np.ndarray]):
  return np.count_nonzero(engine_inputs[0][0])
```

## Tutorials
There are serveral examples integrating Prometheus and DeepSparse logging available on GitHub:
- [Server with Prometheus / Grafana](https://github.com/neuralmagic/deepsparse/tree/rs-logging-sdk/logging-sdk/tutorial-server-prometheus)
- [Monitoring while running Kubernetes](https://github.com/neuralmagic/deepsparse/tree/rs-logging-sdk/logging-sdk/tutorial-kubernetes-prometheus)