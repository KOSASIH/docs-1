---
title: "Sparsify a Model"
metaTitle: "Sparsify a Model"
metaDescription: "Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/get-started/sparsify-a-model.mdx"
index: 4000
---

# Sparsify a Model

SparseML contains many state-of-the-art, advanced sparsification algorithms, including pruning, distillation, and quantization techniques.
These algorithms are built on top of sparsification recipes enabling easy integration into ML pipelines to sparsify most neural networks.
In addition to integrating into custom pipelines, it contains integrations with many popular ML repositories.
With these integrations, creating a recipe is all needed to sparsify any model the repos contain.

Aside from sparsification algorithms, SparseML contains generic export pathways for performant deployments.
These export pathways ensure the model saves in the correct format and rewrites the inference graphs for performance, such as quantized operator folding.
The results are simple to export CLIs and APIs that guarantee performance for sparsified models in their given deployment environment.

## Example Use Cases

The docs below walk through use cases leveraging SparseML to sparsify models with recipes and exporting for performant inference.

<LinkCards>
  <LinkCard href="./supported-integrations" heading="Supported Integrations">
    Example creating a recipe and utilizing it with supported SparseML integrations to create sparsified models.
  </LinkCard>

  <LinkCard href="./custom-integrations" heading="Custom Integrations">
    Example enabling SparseML sparsification techniques with a custom ML pipeline to create sparsified models.
  </LinkCard>
</LinkCards>

## Other Use Cases

More documentation, models, use cases, and examples are continually being added.
If you don't see one you're interested in, search the [DeepSparse Github repo](https://github.com/neuralmagic/deepsparse), the [SparseML Github repo](https://github.com/neuralmagic/sparseml), the [SparseZoo website](https://sparsezoo.neuralmagic.com/), or ask in the [Neural Magic Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ).
