---
title: "Home"
metaTitle: "Neural Magic Docs"
metaDescription: "Documentation for the Neural Magic Deep Sparse Platform: a suite of software components to train and deploy sparsified deep learning models performantly on your data"
index: 0
---

# Neural Magic DeepSparse Platform

Optimize and deploy models with GPU-class performance on commodity CPUs with the DeepSparse Platform.

## Why CPU-only Deployments?

***Software-only deployments on commodity hardware are simple and scalable.***

Because DeepSparse reaches GPU-class performance on CPUs, users no longer need to tether deployments to special
accelerators to reach the performance needed for production. Free from specialized accelerators, deployments can take advantage 
of the simplicity and scalability of software-only deployment:
- Scale vertically from 2 to 192 cores, tailoring the footprint to an app's needs
- Scale horizontally with standard Kubernetes, including using services like EKS/GKE
- Deploy the same model/runtime on any hardware from Intel to AMD to ARM and from cloud to data center to edge, including on pre-existing systems
- No wrestling with drivers, operator support, and compatibility issues

Simply put, deep learning deployments no longer need to choose between the performance of GPUs and simplicty of software!

## DeepSparse Platform Overview

The DeepSparse Platform contains three libraries that support two major workflows.

### Step 1: Optimize a Model for Inference

**SparseML** and **SparseZoo** work together to optimize models trained on custom data for inference with
techniques like pruning and quantization (which we call "sparsity").

- [SparseML](/products/sparseml) is an open-source library that extends PyTorch and TensorFlow to simplify the process of 
  applying training-aware sparsity algorithms. Via simple CLI scipts or 5 lines of code, users can apply training-aware 
  sparsification algorithms to any model or transfer learn from pre-sparsified versions of foundation models 
  like ResNet, YOLOv5, or BERT.

- [SparseZoo](/products/sparsezoo) is an open-source repository of pre-sparsified models
  (for example, sparse ResNet-50 has 95% of weights set to 0 while maintaining 99% of the baseline accuracy). SparseZoo is integrated with 
  SparseML, making it trival for users to fine-tune from sparse model (which we call "Sparse Transfer Learning") onto their data.

### Step 2: Deploy a Model on CPUs

**DeepSparse** runs the optimized models from step 1 (or any unoptimized model in ONNX format) fast on commodity CPUs.

- [DeepSparse](/products/deepsparse) is a CPU-only runtime, wrapped with APIs for deploying deep learnings models. 
  The runtime offers GPU-class performance on commodity hardware for inference optimized models. 
  The APIs enable users (1) to integrate DL into an application via pre-made pipelines exposed as REST endpoints
  or Python classes and (2) to monitor the performance of models running in production.

## Contents

The documentation is organized into five sections:

- **GET STARTED** provides install instructions and a tour of major functionality
- **USE CASES** walks through detailed examples using SparseML and DeepSparse
- **USER GUIDE** shows more advanced functionality with specific tasks
- **PRODUCTS** describes all classes and functions
- **EXAMPLES** includes end-to-end examples utilizing the DeepSparse Platform

## External Resources

✅ Join our [community](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) if you need any help
or [subscribe](https://neuralmagic.com/deep-sparse-community/#subscribe) for regular Neural Magic email updates.

✅ Check out our [GitHub repositories](https://github.com/neuralmagic) and give us a ⭐ as we appreciate the community support!

✅ Help us improve this [documentation](https://github.com/neuralmagic/docs).
