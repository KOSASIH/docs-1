

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>sparseml.pytorch.utils.mfac_helpers &mdash; SparseML 0.10.1.20220216 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/nm-theme-adjustment.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> SparseML
          

          
            
            <img src="../../../../_static/icon-sparseml.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.10
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/code.html">Sparsification Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/recipes.html">Sparsification Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../source/onnx_export.html">ONNX Export</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/sparseml.html">sparseml package</a></li>
</ul>
<p class="caption"><span class="caption-text">Connect Online</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">Bugs, Feature Requests</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discuss.neuralmagic.com/">Support, General Q&amp;A Forums</a></li>
<li class="toctree-l1"><a class="reference external" href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ">Deep Sparse Community Slack</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic">Neural Magic GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.neuralmagic.com">Neural Magic Docs</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">SparseML</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>sparseml.pytorch.utils.mfac_helpers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sparseml.pytorch.utils.mfac_helpers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2021 - present / Neuralmagic, Inc. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing,</span>
<span class="c1"># software distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Helper functions for performing Matrix-Free Approximate Curvature (M-FAC)</span>
<span class="sd">pruning.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Generator</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel.parallel_apply</span> <span class="kn">import</span> <span class="n">parallel_apply</span>

<span class="kn">from</span> <span class="nn">sparseml.pytorch.utils</span> <span class="kn">import</span> <span class="n">tensors_module_forward</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;GradSampler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MFACOptions&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FisherInverse&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FisherInverseFast&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FisherInverseFastBlock&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FisherInverseFastPageSwap&quot;</span><span class="p">,</span>
    <span class="s2">&quot;compute_hessian_inv&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="GradSampler"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.GradSampler">[docs]</a><span class="k">class</span> <span class="nc">GradSampler</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for computing gradient samples for a Model given a sample data loader and</span>
<span class="sd">    loss function.</span>

<span class="sd">    :param data_loader: iterator of data samples to use as model inputs and their loss</span>
<span class="sd">        targets. Samples can either be single tensors as model input or a list of</span>
<span class="sd">        inputs and should be iterated in tuples with their targets</span>
<span class="sd">    :param loss_fn: function to be called on model outputs to compute the loss at</span>
<span class="sd">        each step</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data_loader</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_loader</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;data_loader for GradSampler must be Iterable, received object of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;loss_fn for GradSampler must be callable, given input &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;with type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span> <span class="o">=</span> <span class="n">data_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>

<div class="viewcode-block" id="GradSampler.module_forward"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.GradSampler.module_forward">[docs]</a>    <span class="k">def</span> <span class="nf">module_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param module: module to perform forward pass with</span>
<span class="sd">        :param data: single data sample to pass to module</span>
<span class="sd">        :return: output(s) of the module forward pass</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">tensors_module_forward</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradSampler.module_backward"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.GradSampler.module_backward">[docs]</a>    <span class="k">def</span> <span class="nf">module_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes module loss based on the given module outputs, target data and loss</span>
<span class="sd">        function</span>

<span class="sd">        :param module_outputs: outputs of a forward pass from a module</span>
<span class="sd">        :param targets: target outputs for the module to be used for the loss function</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">module_outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span></div>

<div class="viewcode-block" id="GradSampler.iter_module_backwards"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.GradSampler.iter_module_backwards">[docs]</a>    <span class="k">def</span> <span class="nf">iter_module_backwards</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">num_grads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param module: module to compute gradients for</span>
<span class="sd">        :param num_grads: number of gradient samples to compute</span>
<span class="sd">        :return: generator that yields after every gradient is computed with the index</span>
<span class="sd">            of the gradient sample number</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">computed_grads</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="n">computed_grads</span> <span class="o">&lt;</span> <span class="n">num_grads</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">sample</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_loader</span><span class="p">:</span>
                <span class="c1"># run sample forward and backwards pass</span>
                <span class="n">model_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">module_backward</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

                <span class="c1"># yield so gradients can be collected</span>
                <span class="n">computed_grads</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">yield</span> <span class="n">computed_grads</span>

                <span class="k">if</span> <span class="n">computed_grads</span> <span class="o">&gt;=</span> <span class="n">num_grads</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="n">module</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="MFACOptions"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.MFACOptions">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">MFACOptions</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Options for running the Matrix-Free Approxmiate Curvature (M-FAC) algorithm</span>

<span class="sd">    :param num_grads: number of gradients to store in buffer for Fisher computation.</span>
<span class="sd">        can be an int where that constant value will be used throughout pruning or a</span>
<span class="sd">        dictionary of float sparsity values to the number of gradients that should be</span>
<span class="sd">        stored when that sparsity level (between 0.0 and 1.0) is reached. If a</span>
<span class="sd">        dictionary, then 0.0 must be included as a key for the base number of gradients</span>
<span class="sd">        to store (i.e. {0: 64, 0.5: 128, 0.75: 256}). Default is 64</span>
<span class="sd">    :param damp: dampening factor, default is 1e-5</span>
<span class="sd">    :param grads_device: device to store the gradient buffer on. Default is &quot;cpu&quot;</span>
<span class="sd">    :param fisher_block_size: optional value to enable blocked computation of the</span>
<span class="sd">        Fisher matrix. Blocks will be formed consecutively along the diagonal. If</span>
<span class="sd">        None, blocked computation is not used. Default is 2000</span>
<span class="sd">    :param num_pages: number of pages to break the gradient samples into for GPU</span>
<span class="sd">        computation. Only available when blocked computation is not enabled.</span>
<span class="sd">        Default is 1</span>
<span class="sd">    :param available_gpus: list of GPU device names to perform computation on. Default</span>
<span class="sd">        is empty</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_grads</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">damp</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">grads_device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="n">fisher_block_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2000</span>
    <span class="n">num_pages</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># break computation into pages when block size is None</span>
    <span class="n">available_gpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>

<div class="viewcode-block" id="MFACOptions.get_num_grads_for_sparsity"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.MFACOptions.get_num_grads_for_sparsity">[docs]</a>    <span class="k">def</span> <span class="nf">get_num_grads_for_sparsity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_grads</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_grads</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparsity</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
            <span class="n">sparsity</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sparsity</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">sparsity</span><span class="p">)</span>

        <span class="n">sparsity_thresholds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_grads</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">key</span><span class="p">)))</span>
        <span class="k">if</span> <span class="mf">0.0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sparsity_thresholds</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Dictionary of sparsity thresholds to number of grads given for &quot;</span>
                <span class="s2">&quot;MFACOptions.num_grads, but 0 not included as a sparsity threshold. &quot;</span>
                <span class="s2">&quot;0.0 must be included as a sparsity threshold. Given thresholds &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">sparsity_thresholds</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="p">(</span>
            <span class="n">idx</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">sparsity_thresholds</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">float</span><span class="p">(</span><span class="n">sparsity_thresholds</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">sparsity</span>
        <span class="p">):</span>
            <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_grads</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_grads</span><span class="p">[</span><span class="n">sparsity_thresholds</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span></div></div>


<div class="viewcode-block" id="FisherInverse"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverse">[docs]</a><span class="k">class</span> <span class="nc">FisherInverse</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Abstract class for working with the inverse Fisher information matrix. Storing</span>
<span class="sd">    the full matrix is not a requirement.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="FisherInverse.diag"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverse.diag">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :return: the entries along the diagonal entries of the inverse Fisher matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="FisherInverse.mul"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverse.mul">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param x: tensor to multiply with the inverse Fisher matrix</span>
<span class="sd">        :return: the matrix multiplied value of x and the inverse Fisher matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="FisherInverseFast"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFast">[docs]</a><span class="k">class</span> <span class="nc">FisherInverseFast</span><span class="p">(</span><span class="n">FisherInverse</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base implementation of computing the inverse Fisher matrix values based on the</span>
<span class="sd">    M-FAC paper. Takes O(d * m) memory and O(d * m^2) time to initialize where d</span>
<span class="sd">    is the number of parameters and m is the number of gradient samples</span>

<span class="sd">    :param grads: tensor of gradient samples to compute the inverse Fisher product</span>
<span class="sd">        with. Dimension should be (num_samples, num_parameters)</span>
<span class="sd">    :param damp: the dampening factor. Default is 1e-5</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">damp</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">damp</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span> <span class="o">=</span> <span class="n">grads</span>  <span class="c1"># placeholder for grads^T * H^-1 * grads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
        <span class="p">)</span>

        <span class="n">grad_sample</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">grad_sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">+</span> <span class="n">grad_sample</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">):</span>
            <span class="n">grad_sample</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">grad_sample</span>
            <span class="n">mul</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[:</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">grad_sample</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[:</span><span class="n">idx</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">mul</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[:</span><span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">+</span> <span class="n">grad_sample</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>

<div class="viewcode-block" id="FisherInverseFast.diag"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFast.diag">[docs]</a>    <span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :return: the entries along the diagonal entries of the inverse Fisher matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">):</span>
            <span class="n">res</span> <span class="o">-=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">res</span></div>

<div class="viewcode-block" id="FisherInverseFast.mul"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFast.mul">[docs]</a>    <span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param x: tensor to multiply with the inverse Fisher matrix</span>
<span class="sd">        :return: the matrix multiplied value of x and the inverse Fisher matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">mul</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span>
        <span class="n">res</span> <span class="o">-=</span> <span class="n">mul</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span></div>

<div class="viewcode-block" id="FisherInverseFast.to"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFast.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param device: device to move intermediate results to</span>
<span class="sd">        :return: device movement done in place, returns a copy of this object as well</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># in-place</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="FisherInverseFastBlock"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFastBlock">[docs]</a><span class="k">class</span> <span class="nc">FisherInverseFastBlock</span><span class="p">(</span><span class="n">FisherInverse</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of computing the inverse Fisher matrix values based on the</span>
<span class="sd">    M-FAC paper using a given block size to break up computation. Individual</span>
<span class="sd">    blocks must fit into GPU memory.</span>

<span class="sd">    :param grads: tensor of gradient samples to compute the inverse Fisher product</span>
<span class="sd">        with. Dimension should be (num_samples, num_parameters)</span>
<span class="sd">    :param block_size: size of blocks to form along diagonal of the Fisher matrix</span>
<span class="sd">    :param damp: the dampening factor. Default is 1e-5</span>
<span class="sd">    :param devices: list of GPU device ids to use for computation. Default is to use cpu</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">damp</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span> <span class="o">=</span> <span class="n">devices</span> <span class="ow">or</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_inv_blocks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

        <span class="c1"># run block computations in parallel across devices</span>
        <span class="n">threads</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)</span>
        <span class="n">thread_fisher_inv_blocks</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_compute_fisher_inv_block</span><span class="p">(</span><span class="n">block_start_idx</span><span class="p">,</span> <span class="n">thread_idx_</span><span class="p">):</span>
            <span class="n">block</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">grads</span><span class="p">[:,</span> <span class="n">block_start_idx</span> <span class="p">:</span> <span class="p">(</span><span class="n">block_start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_block_size</span><span class="p">)]</span>
                <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">[</span><span class="n">thread_idx_</span><span class="p">])</span>
                <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">fisher_inv_block</span> <span class="o">=</span> <span class="n">FisherInverseFast</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">damp</span><span class="o">=</span><span class="n">damp</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
                <span class="c1"># ignoring flake8 warning since thread_fisher_inv_blocks is safely</span>
                <span class="c1"># deleted after all calls to _compute_fisher_inv_block</span>
                <span class="n">thread_fisher_inv_blocks</span><span class="p">[</span><span class="n">thread_idx_</span><span class="p">]</span> <span class="o">=</span> <span class="n">fisher_inv_block</span>  <span class="c1"># noqa: F821</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">off</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">grads</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_block_size</span><span class="p">)):</span>
            <span class="c1"># create thread</span>
            <span class="n">thread_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)</span>
            <span class="n">threads</span><span class="p">[</span><span class="n">thread_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
                <span class="n">target</span><span class="o">=</span><span class="n">_compute_fisher_inv_block</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">off</span><span class="p">,</span> <span class="n">thread_idx</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="c1"># run all threads on last iteration or when devices will be full</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">thread_idx</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="ow">or</span> <span class="n">off</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_block_size</span> <span class="o">&gt;=</span> <span class="n">grads</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">[:</span> <span class="p">(</span><span class="n">thread_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]:</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">[:</span> <span class="p">(</span><span class="n">thread_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]:</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_inv_blocks</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">fisher_inv_block</span> <span class="ow">in</span> <span class="n">thread_fisher_inv_blocks</span><span class="p">[</span>
                            <span class="p">:</span> <span class="p">(</span><span class="n">thread_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="p">]</span>
                        <span class="k">if</span> <span class="n">fisher_inv_block</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">]</span>
                <span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># free h_inv_blocks from GPU memory</span>
        <span class="k">del</span> <span class="n">thread_fisher_inv_blocks</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

<div class="viewcode-block" id="FisherInverseFastBlock.diag"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFastBlock.diag">[docs]</a>    <span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :return: the entries along the diagonal entries of the inverse Fisher matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">fisher_inv_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fisher_inv_blocks</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">[</span><span class="n">idx</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)]</span>
            <span class="n">fisher_inv_block</span> <span class="o">=</span> <span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">diag</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
            <span class="c1"># free GPU mem</span>
            <span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">res</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span></div>

<div class="viewcode-block" id="FisherInverseFastBlock.mul"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFastBlock.mul">[docs]</a>    <span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param x: tensor to multiply with the inverse Fisher matrix</span>
<span class="sd">        :return: the matrix multiplied value of x and the inverse Fisher matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">fisher_inv_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fisher_inv_blocks</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">[</span><span class="n">idx</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)]</span>
            <span class="n">fisher_inv_block</span> <span class="o">=</span> <span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">x_block</span> <span class="o">=</span> <span class="n">x</span><span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">_block_size</span> <span class="o">*</span> <span class="n">idx</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_block_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                <span class="n">device</span>
            <span class="p">)</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x_block</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>

            <span class="c1"># free GPU mem</span>
            <span class="n">fisher_inv_block</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="FisherInverseFastPageSwap"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFastPageSwap">[docs]</a><span class="k">class</span> <span class="nc">FisherInverseFastPageSwap</span><span class="p">(</span><span class="n">FisherInverse</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of computing the inverse Fisher matrix values based on the</span>
<span class="sd">    M-FAC paper using a given page size to break up computation across samples.</span>
<span class="sd">    Pages of gradients must fit into GPU memory.</span>

<span class="sd">    :param grads: tensor of gradient samples to compute the inverse Fisher product</span>
<span class="sd">        with. Dimension should be (num_samples, num_parameters)</span>
<span class="sd">    :param damp: the dampening factor. Default is 1e-5</span>
<span class="sd">    :param num_pages: number of pages to break gradient samples into. the number of</span>
<span class="sd">        gradients must be divisible by num_pages</span>
<span class="sd">    :param devices: list of GPU device ids to use for computation. Default is to use cpu</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">damp</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">num_pages</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">(),</span> <span class="p">(</span>
            <span class="s2">&quot;CUDA enabled device not available, &quot;</span>
            <span class="s2">&quot;but is required for using FisherInverseFastPageSwap&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span> <span class="o">=</span> <span class="n">devices</span> <span class="ow">or</span> <span class="p">[</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># for computations that fit on single GPU</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span> <span class="o">=</span> <span class="n">grads</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">damp</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">&lt;</span> <span class="n">num_pages</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_grads cannot be smaller than num_pages&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">%</span> <span class="n">num_pages</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;num_grads </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="si">}</span><span class="s2"> must be divisible by &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;num_pages </span><span class="si">{</span><span class="n">num_pages</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">//</span> <span class="n">num_pages</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params_per_device</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span> <span class="o">=</span> <span class="n">grads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># compute fisher inverse for first page across all GPUs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_comp_first_page</span><span class="p">()</span>

        <span class="c1"># run updates to fisher inverse on main GPU for remaining pages</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_update_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">page_offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_comp_page</span><span class="p">(</span><span class="n">page_offset</span><span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_update_buffer</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>

<div class="viewcode-block" id="FisherInverseFastPageSwap.diag"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFastPageSwap.diag">[docs]</a>    <span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :return: the entries along the diagonal entries of the inverse Fisher matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">page_offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
            <span class="n">hinv_g_page</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span>
                <span class="n">page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">),</span> <span class="p">:</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">page_sample_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
                <span class="n">res</span> <span class="o">-=</span> <span class="p">(</span><span class="n">hinv_g_page</span><span class="p">[</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span>
                    <span class="n">page_sample_idx</span> <span class="o">+</span> <span class="n">page_offset</span>
                <span class="p">]</span>
            <span class="k">del</span> <span class="n">hinv_g_page</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span></div>

<div class="viewcode-block" id="FisherInverseFastPageSwap.mul"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.FisherInverseFastPageSwap.mul">[docs]</a>    <span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param x: tensor to multiply with the inverse Fisher matrix</span>
<span class="sd">        :return: the matrix multiplied value of x and the inverse Fisher matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">page_offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
            <span class="n">hinv_g_page</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span>
                <span class="n">page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">),</span> <span class="p">:</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>
            <span class="n">mul</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hinv_g_page</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">)]</span>
            <span class="p">)</span>
            <span class="n">res</span> <span class="o">-=</span> <span class="n">mul</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hinv_g_page</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">hinv_g_page</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span></div>

    <span class="k">def</span> <span class="nf">_comp_first_page</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move first page value to devices across GPUs</span>
        <span class="k">def</span> <span class="nf">_get_first_page_on_device</span><span class="p">(</span><span class="n">params_idx</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span>
                <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">,</span>
                <span class="n">params_idx</span> <span class="p">:</span> <span class="p">(</span><span class="n">params_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_per_device</span><span class="p">),</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">first_page_hinv_g_dist</span> <span class="o">=</span> <span class="n">parallel_apply</span><span class="p">(</span>
            <span class="p">[</span><span class="n">_get_first_page_on_device</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">),</span>
            <span class="nb">list</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_per_device</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># compute value for first gradient sample</span>
        <span class="k">def</span> <span class="nf">_process_first_sample</span><span class="p">(</span><span class="n">first_page_hinv_g</span><span class="p">):</span>
            <span class="n">first_grad</span> <span class="o">=</span> <span class="n">first_page_hinv_g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">first_page_hinv_g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">first_grad</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">first_grad</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">first_page_hinv_g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="n">parallel_apply</span><span class="p">(</span>
            <span class="p">[</span><span class="n">_process_first_sample</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">),</span>
            <span class="n">first_page_hinv_g_dist</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span>

        <span class="k">for</span> <span class="n">sample_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
            <span class="c1"># update the other page gradients in parallel with two steps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mul_tmp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sample_idx</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sample_grads_dist</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">)</span>  <span class="c1"># type: List[Tensor]</span>

            <span class="k">def</span> <span class="nf">_calc_mul_update_dist</span><span class="p">(</span><span class="n">device_idx</span><span class="p">,</span> <span class="n">hinv_g_shard</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sample_grads_dist</span><span class="p">[</span><span class="n">device_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">hinv_g_shard</span><span class="p">[</span>
                    <span class="n">sample_idx</span><span class="p">,</span> <span class="p">:</span>
                <span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">hinv_g_shard</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_grads_dist</span><span class="p">[</span><span class="n">device_idx</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_mul_tmp</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="n">hinv_g_shard</span><span class="p">[:</span><span class="n">sample_idx</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_grads_dist</span><span class="p">[</span><span class="n">device_idx</span><span class="p">])</span>
                    <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="n">parallel_apply</span><span class="p">(</span>
                <span class="p">[</span><span class="n">_calc_mul_update_dist</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">),</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">first_page_hinv_g_dist</span><span class="p">)),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mul_tmp</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[:</span><span class="n">sample_idx</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">_apply_mul_update_dist</span><span class="p">(</span><span class="n">device_idx</span><span class="p">,</span> <span class="n">hinv_g_shard</span><span class="p">):</span>
                <span class="n">hinv_g_shard</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mul_tmp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="n">hinv_g_shard</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hinv_g_shard</span><span class="p">[:</span><span class="n">sample_idx</span><span class="p">,</span> <span class="p">:])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_sample_grads_dist</span><span class="p">[</span><span class="n">device_idx</span><span class="p">]</span>
                    <span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hinv_g_shard</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">,</span> <span class="p">:])</span>
                    <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="n">parallel_apply</span><span class="p">(</span>
                <span class="p">[</span><span class="n">_apply_mul_update_dist</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_devices</span><span class="p">),</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">first_page_hinv_g_dist</span><span class="p">)),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mul_tmp</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_grads_dist</span>

        <span class="k">def</span> <span class="nf">_update_main_hinv_g</span><span class="p">(</span><span class="n">shard_param_idx</span><span class="p">,</span> <span class="n">hinv_g_shard</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span>
                <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">,</span>
                <span class="n">shard_param_idx</span> <span class="p">:</span> <span class="p">(</span><span class="n">shard_param_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_per_device</span><span class="p">),</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">hinv_g_shard</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="n">parallel_apply</span><span class="p">(</span>
            <span class="p">[</span><span class="n">_update_main_hinv_g</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_page_hinv_g_dist</span><span class="p">),</span>
            <span class="nb">list</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span>
                    <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_per_device</span><span class="p">),</span>
                    <span class="n">first_page_hinv_g_dist</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="k">del</span> <span class="n">first_page_hinv_g_dist</span>

    <span class="k">def</span> <span class="nf">_comp_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">page_offset</span><span class="p">):</span>
        <span class="c1"># update fisher update buffer</span>
        <span class="k">for</span> <span class="n">prev_page_offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">page_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
            <span class="n">prev_page_hinv_g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span>
                <span class="n">prev_page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">+</span> <span class="n">prev_page_offset</span><span class="p">),</span> <span class="p">:</span>
            <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">page_sample_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
                <span class="n">grad_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">page_sample_idx</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span>
                <span class="p">)</span>
                <span class="n">mul</span> <span class="o">=</span> <span class="n">prev_page_hinv_g</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">grad_sample</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span>
                    <span class="n">prev_page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">+</span> <span class="n">prev_page_offset</span><span class="p">)</span>
                <span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>
                <span class="n">mul</span> <span class="o">=</span> <span class="n">mul</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">prev_page_hinv_g</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">prev_page_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_update_buffer</span><span class="p">[</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_damp</span> <span class="o">*</span> <span class="n">grad_sample</span> <span class="o">-</span> <span class="n">mul</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_update_buffer</span><span class="p">[</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">mul</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">prev_page_hinv_g</span>

        <span class="c1"># move buffer to main GPU and update the fisher inv state</span>
        <span class="n">fisher_inv_buf_gpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fisher_update_buffer</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>

        <span class="n">grad_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">page_offset</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">page_offset</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">+</span> <span class="n">grad_sample</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
            <span class="n">fisher_inv_buf_gpu</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">page_sample_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span><span class="p">):</span>
            <span class="n">grad_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span><span class="n">page_sample_idx</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span><span class="p">)</span>
            <span class="n">mul</span> <span class="o">=</span> <span class="n">fisher_inv_buf_gpu</span><span class="p">[:</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                <span class="n">grad_sample</span>
            <span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span><span class="n">page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="n">page_sample_idx</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">)]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_gpu0</span>
            <span class="p">)</span>
            <span class="n">fisher_inv_buf_gpu</span><span class="p">[</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-=</span> <span class="n">mul</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                <span class="n">fisher_inv_buf_gpu</span><span class="p">[:</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_denom</span><span class="p">[</span>
                <span class="n">page_sample_idx</span> <span class="o">+</span> <span class="n">page_offset</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">+</span> <span class="n">grad_sample</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
                <span class="n">fisher_inv_buf_gpu</span><span class="p">[</span><span class="n">page_sample_idx</span><span class="p">,</span> <span class="p">:]</span>
            <span class="p">)</span>

        <span class="c1"># update main tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hinv_g</span><span class="p">[</span>
            <span class="n">page_offset</span> <span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples_per_page</span> <span class="o">+</span> <span class="n">page_offset</span><span class="p">),</span> <span class="p">:</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">fisher_inv_buf_gpu</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">fisher_inv_buf_gpu</span></div>


<div class="viewcode-block" id="compute_hessian_inv"><a class="viewcode-back" href="../../../../api/sparseml.pytorch.utils.html#sparseml.pytorch.optim.mask_pruning_scorer.compute_hessian_inv">[docs]</a><span class="k">def</span> <span class="nf">compute_hessian_inv</span><span class="p">(</span>
    <span class="n">grads</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mfac_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MFACOptions</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FisherInverse</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :param grads: tensor of gradient samples to compute the Hessian inverse</span>
<span class="sd">        representation with. Should have shape (num_samples, num_parameters)</span>
<span class="sd">    :param mfac_options: MFACOptions object specifying how to perform the computations</span>
<span class="sd">    :return: FisherInverse object with access to the diagonal multiplication of the</span>
<span class="sd">        Fisher approximation of the Hessian inverse</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">mfac_options</span><span class="p">:</span>
        <span class="n">mfac_options</span> <span class="o">=</span> <span class="n">MFACOptions</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">mfac_options</span><span class="o">.</span><span class="n">fisher_block_size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FisherInverseFastBlock</span><span class="p">(</span>
            <span class="n">grads</span><span class="p">,</span>
            <span class="n">mfac_options</span><span class="o">.</span><span class="n">fisher_block_size</span><span class="p">,</span>
            <span class="n">damp</span><span class="o">=</span><span class="n">mfac_options</span><span class="o">.</span><span class="n">damp</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">mfac_options</span><span class="o">.</span><span class="n">available_gpus</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">mfac_options</span><span class="o">.</span><span class="n">available_gpus</span> <span class="ow">or</span> <span class="n">mfac_options</span><span class="o">.</span><span class="n">num_pages</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FisherInverseFastPageSwap</span><span class="p">(</span>
            <span class="n">grads</span><span class="p">,</span>
            <span class="n">damp</span><span class="o">=</span><span class="n">mfac_options</span><span class="o">.</span><span class="n">damp</span><span class="p">,</span>
            <span class="n">num_pages</span><span class="o">=</span><span class="n">mfac_options</span><span class="o">.</span><span class="n">num_pages</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">mfac_options</span><span class="o">.</span><span class="n">available_gpus</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FisherInverseFast</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">damp</span><span class="o">=</span><span class="n">mfac_options</span><span class="o">.</span><span class="n">damp</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.10.1
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../../../v0.10.0/index.html">v0.10.0</a></dd>
      <dd><a href="mfac_helpers.html">v0.10.1</a></dd>
      <dd><a href="../../../../../v0.3.0/index.html">v0.3.0</a></dd>
      <dd><a href="../../../../../v0.3.1/index.html">v0.3.1</a></dd>
      <dd><a href="../../../../../v0.4.0/index.html">v0.4.0</a></dd>
      <dd><a href="../../../../../v0.5.0/index.html">v0.5.0</a></dd>
      <dd><a href="../../../../../v0.5.1/index.html">v0.5.1</a></dd>
      <dd><a href="../../../../../v0.6.0/index.html">v0.6.0</a></dd>
      <dd><a href="../../../../../v0.7.0/index.html">v0.7.0</a></dd>
      <dd><a href="../../../../../v0.8.0/index.html">v0.8.0</a></dd>
      <dd><a href="../../../../../v0.9.0/index.html">v0.9.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../../../../main/index.html">main</a></dd>
    </dl>
  </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>