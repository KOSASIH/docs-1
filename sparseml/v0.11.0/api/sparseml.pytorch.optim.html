<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>sparseml.pytorch.optim package &mdash; SparseML 0.11.0.20220630 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/nm-theme-adjustment.css" type="text/css" /><link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="sparseml.pytorch.sparsification package" href="sparseml.pytorch.sparsification.html" />
    <link rel="prev" title="sparseml.pytorch.nn package" href="sparseml.pytorch.nn.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> SparseML<img src="../_static/icon-sparseml.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.11
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">General</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../source/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/code.html">Sparsification Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/recipes.html">Sparsification Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/onnx_export.html">ONNX Export</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="sparseml.html">sparseml package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="sparseml.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="sparseml.benchmark.html">sparseml.benchmark package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.deepsparse.html">sparseml.deepsparse package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.framework.html">sparseml.framework package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.keras.html">sparseml.keras package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.onnx.html">sparseml.onnx package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.optim.html">sparseml.optim package</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="sparseml.pytorch.html">sparseml.pytorch package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="sparseml.pytorch.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="sparseml.pytorch.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch.base">sparseml.pytorch.base module</a></li>
<li class="toctree-l4"><a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.sparsification.html">sparseml.sparsification package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.tensorflow_v1.html">sparseml.tensorflow_v1 package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.utils.html">sparseml.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml.base">sparseml.base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml.log">sparseml.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml.version">sparseml.version module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml">Module contents</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Connect Online</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">Bugs, Feature Requests</a></li>
<li class="toctree-l1"><a class="reference external" href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ">Deep Sparse Community Slack</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic">Neural Magic GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.neuralmagic.com">Neural Magic Docs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseML</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="sparseml.html">sparseml package</a> &raquo;</li>
          <li><a href="sparseml.pytorch.html">sparseml.pytorch package</a> &raquo;</li>
      <li>sparseml.pytorch.optim package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/sparseml.pytorch.optim.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="sparseml-pytorch-optim-package">
<h1>sparseml.pytorch.optim package<a class="headerlink" href="#sparseml-pytorch-optim-package" title="Permalink to this headline"></a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="module-sparseml.pytorch.optim.analyzer_as">
<span id="sparseml-pytorch-optim-analyzer-as-module"></span><h2>sparseml.pytorch.optim.analyzer_as module<a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_as" title="Permalink to this headline"></a></h2>
<p>Code related to analyzing activation sparsity within PyTorch neural networks.
More information can be found in the paper
<a class="reference external" href="https://arxiv.org/abs/1705.01626">here</a>.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.analyzer_as.</span></code><code class="sig-name descname"><span class="pre">ASResultType</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ASResultType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Result type to track for activation sparsity.</p>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sample">
<code class="sig-name descname"><span class="pre">inputs_sample</span></code><em class="property"> <span class="pre">=</span> <span class="pre">'inputs_sample'</span></em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sparsity">
<code class="sig-name descname"><span class="pre">inputs_sparsity</span></code><em class="property"> <span class="pre">=</span> <span class="pre">'inputs_sparsity'</span></em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sample">
<code class="sig-name descname"><span class="pre">outputs_sample</span></code><em class="property"> <span class="pre">=</span> <span class="pre">'outputs_sample'</span></em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sparsity">
<code class="sig-name descname"><span class="pre">outputs_sparsity</span></code><em class="property"> <span class="pre">=</span> <span class="pre">'outputs_sparsity'</span></em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.analyzer_as.</span></code><code class="sig-name descname"><span class="pre">ModuleASAnalyzer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_inputs_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_outputs_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_sample_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs_sample_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An analyzer implementation used to monitor the activation sparsity with a module.
Generally used to monitor an individual layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – The module to analyze activation sparsity for</p></li>
<li><p><strong>dim</strong> – Any dims within the tensor such as across batch,
channel, etc. Ex: 0 for batch, 1 for channel, [0, 1] for batch and channel</p></li>
<li><p><strong>track_inputs_sparsity</strong> – True to track the input sparsity to the module,
False otherwise</p></li>
<li><p><strong>track_outputs_sparsity</strong> – True to track the output sparsity to the module,
False otherwise</p></li>
<li><p><strong>inputs_sample_size</strong> – The number of samples to grab from the input tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>outputs_sample_size</strong> – The number of samples to grab from the output tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>enabled</strong> – True to enable the hooks for analyzing and actively track,
False to disable and not track</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.analyze_layers">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">analyze_layers</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_inputs_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_outputs_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_sample_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs_sample_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.analyze_layers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.analyze_layers" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to analyze multiple layers activation sparsity in</p></li>
<li><p><strong>layers</strong> – the names of the layers to analyze (from module.named_modules())</p></li>
<li><p><strong>dim</strong> – Any dims within the tensor such as across batch,
channel, etc. Ex: 0 for batch, 1 for channel, [0, 1] for batch and channel</p></li>
<li><p><strong>track_inputs_sparsity</strong> – True to track the input sparsity to the module,
False otherwise</p></li>
<li><p><strong>track_outputs_sparsity</strong> – True to track the output sparsity to the module,
False otherwise</p></li>
<li><p><strong>inputs_sample_size</strong> – The number of samples to grab from the input tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>outputs_sample_size</strong> – The number of samples to grab from the output tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>enabled</strong> – True to enable the hooks for analyzing and actively track,
False to disable and not track</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of the created analyzers, matches the ordering in layers</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.clear">
<code class="sig-name descname"><span class="pre">clear</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">specific_result_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType"><span class="pre">sparseml.pytorch.optim.analyzer_as.ASResultType</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.clear" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.dim">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">dim</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.dim" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.disable">
<code class="sig-name descname"><span class="pre">disable</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.disable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.disable" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enable">
<code class="sig-name descname"><span class="pre">enable</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.enable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enable" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enabled">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">enabled</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enabled" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sample</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_max">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sample_max</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_max" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_mean">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sample_mean</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_mean" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_min">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sample_min</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_min" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_size">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sample_size</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_size" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_std">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sample_std</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_std" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sparsity</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_max">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sparsity_max</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_max" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_mean">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sparsity_mean</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_mean" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_min">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sparsity_min</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_min" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_std">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">inputs_sparsity_std</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_std" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.module">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">module</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.module" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sample</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_max">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sample_max</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_max" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_mean">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sample_mean</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_mean" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_min">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sample_min</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_min" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_size">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sample_size</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_size" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_std">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sample_std</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_std" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sparsity</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_max">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sparsity_max</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_max" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_mean">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sparsity_mean</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_mean" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_min">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sparsity_min</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_min" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_std">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">outputs_sparsity_std</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_std" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results">
<code class="sig-name descname"><span class="pre">results</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType"><span class="pre">sparseml.pytorch.optim.analyzer_as.ASResultType</span></a></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_max">
<code class="sig-name descname"><span class="pre">results_max</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType"><span class="pre">sparseml.pytorch.optim.analyzer_as.ASResultType</span></a></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_max"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_max" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_mean">
<code class="sig-name descname"><span class="pre">results_mean</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType"><span class="pre">sparseml.pytorch.optim.analyzer_as.ASResultType</span></a></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_mean" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_min">
<code class="sig-name descname"><span class="pre">results_min</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType"><span class="pre">sparseml.pytorch.optim.analyzer_as.ASResultType</span></a></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_min"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_min" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_std">
<code class="sig-name descname"><span class="pre">results_std</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">result_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType"><span class="pre">sparseml.pytorch.optim.analyzer_as.ASResultType</span></a></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_std"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_std" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_inputs_sparsity">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">track_inputs_sparsity</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_inputs_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_outputs_sparsity">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">track_outputs_sparsity</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_outputs_sparsity" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.analyzer_module">
<span id="sparseml-pytorch-optim-analyzer-module-module"></span><h2>sparseml.pytorch.optim.analyzer_module module<a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_module" title="Permalink to this headline"></a></h2>
<p>Code related to monitoring, analyzing, and reporting info for Modules in PyTorch.
Records things like FLOPS, input and output shapes, kernel shapes, etc.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.analyzer_module.</span></code><code class="sig-name descname"><span class="pre">ModuleAnalyzer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An analyzer implementation for monitoring the execution profile and graph of
a Module in PyTorch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to analyze</p></li>
<li><p><strong>enabled</strong> – True to enable the hooks for analyzing and actively track,
False to disable and not track</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.enabled">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">enabled</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.enabled" title="Permalink to this definition"></a></dt>
<dd><p>True if enabled and the hooks for analyzing are active, False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.ks_layer_descs">
<code class="sig-name descname"><span class="pre">ks_layer_descs</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.optim.html#sparseml.optim.analyzer.AnalyzedLayerDesc" title="sparseml.optim.analyzer.AnalyzedLayerDesc"><span class="pre">sparseml.optim.analyzer.AnalyzedLayerDesc</span></a><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer.ks_layer_descs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.ks_layer_descs" title="Permalink to this definition"></a></dt>
<dd><p>Get the descriptions for all layers in the module that support kernel sparsity
(model pruning). Ex: all convolutions and linear layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a list of descriptions for all layers in the module that support ks</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.layer_desc">
<code class="sig-name descname"><span class="pre">layer_desc</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.analyzer.AnalyzedLayerDesc" title="sparseml.optim.analyzer.AnalyzedLayerDesc"><span class="pre">sparseml.optim.analyzer.AnalyzedLayerDesc</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer.layer_desc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.layer_desc" title="Permalink to this definition"></a></dt>
<dd><p>Get a specific layer’s description within the Module.
Set to None to get the overall Module’s description.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> – name of the layer to get a description for,
None for an overall description</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the analyzed layer description for the given name</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.module">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">module</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.module" title="Permalink to this definition"></a></dt>
<dd><p>The module that is being actively analyzed</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.analyzer_pruning">
<span id="sparseml-pytorch-optim-analyzer-pruning-module"></span><h2>sparseml.pytorch.optim.analyzer_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_pruning" title="Permalink to this headline"></a></h2>
<p>Code related to monitoring, analyzing, and reporting the kernel sparsity
(model pruning) for a model’s layers and params.
More info on kernel sparsity can be found <cite>here &lt;https://arxiv.org/abs/1902.09574&gt;</cite> __.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.analyzer_pruning.</span></code><code class="sig-name descname"><span class="pre">ModulePruningAnalyzer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An analyzer implementation monitoring the kernel sparsity of a given
param in a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module containing the param to analyze the sparsity for</p></li>
<li><p><strong>name</strong> – name of the layer, used for tracking</p></li>
<li><p><strong>param_name</strong> – name of the parameter to analyze the sparsity for,
defaults to weight</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.analyze_layers">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">analyze_layers</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer.analyze_layers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.analyze_layers" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to create multiple analyzers for</p></li>
<li><p><strong>layers</strong> – the names of the layers to create analyzer for that are
in the module</p></li>
<li><p><strong>param_name</strong> – the name of the param to monitor within each layer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of analyzers, one for each layer passed in and in the same order</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.module">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">module</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.module" title="Permalink to this definition"></a></dt>
<dd><p>the module containing the param to analyze the sparsity for</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.name">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">name</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.name" title="Permalink to this definition"></a></dt>
<dd><p>name of the layer, used for tracking</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">param</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param" title="Permalink to this definition"></a></dt>
<dd><p>the parameter that is being monitored for kernel sparsity</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_name">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">param_name</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_name" title="Permalink to this definition"></a></dt>
<dd><p>name of the parameter to analyze the sparsity for, defaults to weight</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">param_sparsity</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity" title="Permalink to this definition"></a></dt>
<dd><p>the sparsity of the contained parameter (how many zeros are in it)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity_dim">
<code class="sig-name descname"><span class="pre">param_sparsity_dim</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer.param_sparsity_dim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity_dim" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> – a dimension(s) to calculate the sparsity over, ex over channels</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sparsity of the contained parameter structured according
to the dim passed in</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.tag">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">tag</span></code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.tag" title="Permalink to this definition"></a></dt>
<dd><p>combines the layer name and param name in to a single string
separated by a period</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.manager">
<span id="sparseml-pytorch-optim-manager-module"></span><h2>sparseml.pytorch.optim.manager module<a class="headerlink" href="#module-sparseml.pytorch.optim.manager" title="Permalink to this headline"></a></h2>
<p>Contains base code related to modifier managers: modifier managers handle
grouping modifiers and running them together.
Also handles loading modifiers from yaml files</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.manager.</span></code><code class="sig-name descname"><span class="pre">RecipeManagerStepWrapper</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wrap</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">manager</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A wrapper class to handle wrapping an optimizer or optimizer like object
and override the step function.
The override calls into the ScheduledModifierManager when appropriate and enabled
and then calls step() as usual on the function with the original arguments.
All original attributes and methods are forwarded to the wrapped object
so this class can be a direct substitute for it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>wrap</strong> – The object to wrap the step function and properties for.</p></li>
<li><p><strong>optimizer</strong> – The optimizer used in the training process.</p></li>
<li><p><strong>module</strong> – The model/module used in the training process.</p></li>
<li><p><strong>manager</strong> – The manager to forward lifecycle calls into such as step.</p></li>
<li><p><strong>epoch</strong> – The epoch to start the modifying process at.</p></li>
<li><p><strong>steps_per_epoch</strong> – The number of optimizer steps (batches) in each epoch.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.emulated_step">
<code class="sig-name descname"><span class="pre">emulated_step</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper.emulated_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.emulated_step" title="Permalink to this definition"></a></dt>
<dd><p>Emulated step function to be called in place of step when the
number of steps_per_epoch vary across epochs.
The emulated function should be called to keep the steps_per_epoch thee same.
Does not call into the step function for the wrapped object,
but does call into the manager to increment the steps.</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.loss_update">
<code class="sig-name descname"><span class="pre">loss_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper.loss_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.loss_update" title="Permalink to this definition"></a></dt>
<dd><p>Optional call to update modifiers based on the calculated loss.
Not needed unless one or more of the modifier is using the loss
to make a modification or is modifying the loss itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> – the calculated loss after running a forward pass and loss_fn</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.step">
<code class="sig-name descname"><span class="pre">step</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.step" title="Permalink to this definition"></a></dt>
<dd><p>Override for the step function.
Calls into the base step function with the args and kwargs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – Any args to pass to the wrapped objects step function.</p></li>
<li><p><strong>kwargs</strong> – Any kwargs to pass to the wrapped objects step function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The return, if any, from the wrapped objects step function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped" title="Permalink to this definition"></a></dt>
<dd><p>The object to wrap the step function and properties for.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_epoch">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped_epoch</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_epoch" title="Permalink to this definition"></a></dt>
<dd><p>The current epoch the wrapped object is at.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_manager">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped_manager</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_manager" title="Permalink to this definition"></a></dt>
<dd><p>The manager to forward lifecycle calls into such as step.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_module">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped_module</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_module" title="Permalink to this definition"></a></dt>
<dd><p>The model/module used in the training process.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_optimizer">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped_optimizer</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_optimizer" title="Permalink to this definition"></a></dt>
<dd><p>The optimizer used in the training process.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped_steps</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps" title="Permalink to this definition"></a></dt>
<dd><p>The current number of steps that have been called for
the wrapped object.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps_per_epoch">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">wrapped_steps_per_epoch</span></code><a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps_per_epoch" title="Permalink to this definition"></a></dt>
<dd><p>The number of optimizer steps (batches) in each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.manager.</span></code><code class="sig-name descname"><span class="pre">ScheduledModifierManager</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">modifiers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">sparseml.pytorch.sparsification.modifier.ScheduledModifier</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.manager.BaseManager" title="sparseml.optim.manager.BaseManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.manager.BaseManager</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.sparsification.modifier.Modifier</span></code></p>
<p>The base modifier manager, handles managing multiple ScheduledModifers.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- initialize</div>
<div class="line">- initialize_loggers</div>
<div class="line">- modify</div>
<div class="line">- finalize</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modifiers</strong> – the modifiers to wrap</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.apply">
<code class="sig-name descname"><span class="pre">apply</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.LoggerManager" title="sparseml.pytorch.utils.logger.LoggerManager"><span class="pre">sparseml.pytorch.utils.logger.LoggerManager</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">finalize</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.apply"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.apply" title="Permalink to this definition"></a></dt>
<dd><p>Applies the lifecycle of each stage in the manager/recipe
by calling into initialize and finalize for each modifier for each stage</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the PyTorch model/module to modify</p></li>
<li><p><strong>epoch</strong> – the epoch to apply the modifier at, defaults to math.inf (end)</p></li>
<li><p><strong>loggers</strong> – Optional logger manager to log the modification process to</p></li>
<li><p><strong>finalize</strong> – True to invoke finalize after initialize, False otherwise.
If training after one shot, set finalize=False to keep modifiers applied.</p></li>
<li><p><strong>kwargs</strong> – Optional kwargs to support specific arguments
for individual modifiers (passed to initialize and finalize).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.apply_structure">
<code class="sig-name descname"><span class="pre">apply_structure</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.LoggerManager" title="sparseml.pytorch.utils.logger.LoggerManager"><span class="pre">sparseml.pytorch.utils.logger.LoggerManager</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger"><span class="pre">sparseml.pytorch.utils.logger.BaseLogger</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">finalize</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.apply_structure"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.apply_structure" title="Permalink to this definition"></a></dt>
<dd><p>Initialize/apply the modifier for a given model/module at the given epoch
if the modifier affects the structure of the module such as
quantization, layer pruning, or filter pruning.
Calls into initialize(module, epoch, loggers, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs) if structured.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the PyTorch model/module to modify</p></li>
<li><p><strong>epoch</strong> – the epoch to apply the modifier at, defaults to 0.0 (start)</p></li>
<li><p><strong>loggers</strong> – Optional logger manager to log the modification process to</p></li>
<li><p><strong>finalize</strong> – True to invoke finalize after initialize, False otherwise.
Set finalize to True and epoch to math.inf for one shot application.</p></li>
<li><p><strong>kwargs</strong> – Optional kwargs to support specific arguments
for individual modifiers (passed to initialize and finalize).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.finalize">
<code class="sig-name descname"><span class="pre">finalize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.finalize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.finalize" title="Permalink to this definition"></a></dt>
<dd><p>Handles any finalization of the modifier for the given model/module.
Applies any remaining logic and cleans up any hooks or attachments to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – The model/module to finalize the modifier for.
Marked optional so state can still be cleaned up on delete,
but generally should always be passed in.</p></li>
<li><p><strong>reset_loggers</strong> – True to remove any currently attached loggers (default),
False to keep the loggers attached.</p></li>
<li><p><strong>kwargs</strong> – Optional kwargs to support specific arguments
for individual modifiers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.from_yaml">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">from_yaml</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">sparsezoo.objects.recipe.Recipe</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_modifiers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">sparseml.pytorch.sparsification.modifier.Modifier</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recipe_variables</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.from_yaml"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.from_yaml" title="Permalink to this definition"></a></dt>
<dd><p>Convenience function used to create the manager of multiple modifiers from a
recipe file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> – <p>the path to the recipe file to load the modifier from, or
a SparseZoo model stub to load a recipe for a model stored in SparseZoo.
SparseZoo stubs should be preceded by ‘zoo:’, and can contain an optional
‘?recipe_type=&lt;type&gt;’ parameter. Can also be a SparseZoo Recipe
object. i.e. ‘/path/to/local/recipe.yaml’, ‘zoo:model/stub/path’,
‘zoo:model/stub/path?recipe_type=transfer’. Additionally, a raw</p>
<blockquote>
<div><p>yaml str is also supported in place of a file path.</p>
</div></blockquote>
</p></li>
<li><p><strong>add_modifiers</strong> – additional modifiers that should be added to the
returned manager alongside the ones loaded from the recipe file</p></li>
<li><p><strong>recipe_variables</strong> – additional arguments to override any root variables
in the recipe with (i.e. num_epochs, init_lr)</p></li>
</ul>
</dd>
<dt class="field-even">Metadata</dt>
<dd class="field-even"><p>additional (to the information provided in the recipe) data to be
preserved and utilized in the future - for reproducibility and completeness.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>ScheduledModifierManager() created from the recipe file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize">
<code class="sig-name descname"><span class="pre">initialize</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.LoggerManager" title="sparseml.pytorch.utils.logger.LoggerManager"><span class="pre">sparseml.pytorch.utils.logger.LoggerManager</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger"><span class="pre">sparseml.pytorch.utils.logger.BaseLogger</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.initialize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize" title="Permalink to this definition"></a></dt>
<dd><p>Handles any initialization of the manager for the given model/module.
epoch and steps_per_epoch can optionally be passed in to initialize the manager
and module at a specific point in the training process.
If loggers is not None, will additionally call initialize_loggers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the PyTorch model/module to modify</p></li>
<li><p><strong>epoch</strong> – The epoch to initialize the manager and module at.
Defaults to 0 (start of the training process)</p></li>
<li><p><strong>loggers</strong> – Optional logger manager to log the modification process to</p></li>
<li><p><strong>kwargs</strong> – Optional kwargs to support specific arguments
for individual modifiers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize_loggers">
<code class="sig-name descname"><span class="pre">initialize_loggers</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span> </span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.LoggerManager" title="sparseml.pytorch.utils.logger.LoggerManager"><span class="pre">sparseml.pytorch.utils.logger.LoggerManager</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger"><span class="pre">sparseml.pytorch.utils.logger.BaseLogger</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.initialize_loggers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize_loggers" title="Permalink to this definition"></a></dt>
<dd><p>Handles initializing and setting up the loggers for the contained modifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loggers</strong> – the logger manager to setup this manager with for logging
important info and milestones to</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.load_state_dict">
<code class="sig-name descname"><span class="pre">load_state_dict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.load_state_dict" title="Permalink to this definition"></a></dt>
<dd><p>Loads the given state dict into this manager.
All modifiers that match will be loaded.
If any are missing or extra and strict=True, then will raise a KeyError</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> – dictionary object as generated by this object’s state_dict
function</p></li>
<li><p><strong>strict</strong> – True to raise a KeyError for any missing or extra information in
the state dict, False to ignore</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>IndexError</strong> – If any keys in the state dict do not correspond to a valid
index for this manager and strict=True</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.loss_update">
<code class="sig-name descname"><span class="pre">loss_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.loss_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.loss_update" title="Permalink to this definition"></a></dt>
<dd><p>Optional call that can be made on the optimizer to update the contained
modifiers once loss has been calculated</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – The calculated loss tensor</p></li>
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.modify">
<code class="sig-name descname"><span class="pre">modify</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_optim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_parallel_module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper" title="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper"><span class="pre">sparseml.pytorch.optim.manager.RecipeManagerStepWrapper</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.modify"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.modify" title="Permalink to this definition"></a></dt>
<dd><p>Modify the given module and optimizer for training aware algorithms such as
pruning and quantization.
Initialize must be called first.
After training is complete, finalize should be called.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – The model/module to modify</p></li>
<li><p><strong>optimizer</strong> – The optimizer to modify</p></li>
<li><p><strong>steps_per_epoch</strong> – The number of optimizer steps (batches) in each epoch</p></li>
<li><p><strong>wrap_optim</strong> – Optional object to wrap instead of the optimizer.
Useful for cases like amp (fp16 training) where a it should be wrapped
in place of the original optimizer since it doesn’t always call into
the optimizer.step() function.</p></li>
<li><p><strong>epoch</strong> – Optional epoch that can be passed in to start modifying at.
Defaults to the epoch that was supplied to the initialize function.</p></li>
<li><p><strong>allow_parallel_module</strong> – if False, a DataParallel or
DistributedDataParallel module passed to this function will be unwrapped
to its base module during recipe initialization by referencing
module.module. This is useful so a recipe may reference the base module
parameters instead of the wrapped distributed ones. Set to True to not
unwrap the distributed module. Default is True</p></li>
<li><p><strong>kwargs</strong> – Key word arguments that are passed to the intialize call
if initilaize has not been called yet</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A wrapped optimizer object. The wrapped object makes all the
original properties for the wrapped object available so it can be
used without any additional code changes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_post_step">
<code class="sig-name descname"><span class="pre">optimizer_post_step</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.optimizer_post_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_post_step" title="Permalink to this definition"></a></dt>
<dd><p>Called after the optimizer step happens and weights have updated
Calls into the contained modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_pre_step">
<code class="sig-name descname"><span class="pre">optimizer_pre_step</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.optimizer_pre_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_pre_step" title="Permalink to this definition"></a></dt>
<dd><p>Called before the optimizer step happens (after backward has been called,
before optimizer.step)
Calls into the contained modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.state_dict">
<code class="sig-name descname"><span class="pre">state_dict</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.state_dict" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionary to store any state variables for this manager.
Includes all modifiers nested under this manager as sub keys in the dict.
Only modifiers that a non empty state dict are included.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.update">
<code class="sig-name descname"><span class="pre">update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_updates</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.update" title="Permalink to this definition"></a></dt>
<dd><p>Handles updating the contained modifiers’ states, module, or optimizer
Only calls scheduled_update on the each modifier if modifier.update_ready()</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
<li><p><strong>log_updates</strong> – True to log the updates for each modifier to the loggers,
False to skip logging</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.mask_creator_pruning">
<span id="sparseml-pytorch-optim-mask-creator-pruning-module"></span><h2>sparseml.pytorch.optim.mask_creator_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.mask_creator_pruning" title="Permalink to this headline"></a></h2>
<p>Classes for defining sparsity masks based on model parameters.</p>
<p>NOTE: this file is in the process of being phased out in favor of the
sparsification package. Once all references to mask utils in the optim
package are migrated, this file will be deleted</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">BlockPruningMaskCreator</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">block_shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grouping_fn_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#BlockPruningMaskCreator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>Structured sparsity mask creator that groups the input tensor into blocks of
shape block_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>block_shape</strong> – The shape in and out channel should take in blocks.  Should be
a list of exactly two integers that divide the input tensors evenly on the
channel dimensions.  -1 for a dimension blocks across the entire dimension</p></li>
<li><p><strong>grouping_fn_name</strong> – The name of the torch grouping function to reduce
dimensions by</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor">
<code class="sig-name descname"><span class="pre">group_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#BlockPruningMaskCreator.group_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mean values of the tensor grouped by blocks of shape
self._block_shape</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">DimensionSparsityMaskCreator</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grouping_fn_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_group_idxs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>Structured sparsity mask creator that groups sparsity blocks by the given
dimension(s)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> – The index or list of indices of dimensions to group the mask by or
the type of dims to prune ([‘channel’, ‘filter’])</p></li>
<li><p><strong>grouping_fn_name</strong> – The name of the torch grouping function to reduce
dimensions by. Default is ‘l2’</p></li>
<li><p><strong>tensor_group_idxs</strong> – list of lists of input tensor idxs whose given dimensions
should be scored together. If set, all idxs in the range of provided tensors
must be included in exactly one group (tensors in their own group should be a
list of length 1).  If None, no tensor groups will be used</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.create_sparsity_masks">
<code class="sig-name descname"><span class="pre">create_sparsity_masks</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator.create_sparsity_masks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.create_sparsity_masks" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate masks from based on their contained
values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros) can also be a list where each element is a
sparsity for a tensor in the same position in the tensor list, If global
sparsity is enabled, all values of the sparsity list must be the same</p></li>
<li><p><strong>global_sparsity</strong> – do not set True, unsupported for
DimensionSparsityMaskCreator</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity and all values mapped to the same group have the same
value</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.group_tensor">
<code class="sig-name descname"><span class="pre">group_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator.group_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.group_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mean values of the tensor grouped by the dimension(s) in self._dim</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.set_tensor_group_idxs">
<code class="sig-name descname"><span class="pre">set_tensor_group_idxs</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_group_idxs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator.set_tensor_group_idxs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.set_tensor_group_idxs" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor_group_idxs</strong> – list of lists of input tensor idxs whose given
dimensions should be scored together. If set, all idxs in the range of
provided tensors must be included in exactly one group (tensors in their
own group should be a list of length 1).  If None, no tensor groups will
be used</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.structure_type">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">structure_type</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.structure_type" title="Permalink to this definition"></a></dt>
<dd><p>the type of structure pruned masks this mask creator produces must
be either ‘channel’ or ‘filter’</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.FourBlockMaskCreator">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">FourBlockMaskCreator</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grouping_fn_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#FourBlockMaskCreator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.FourBlockMaskCreator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>semi-structured sparsity mask creator that groups sparsity blocks in groups of four
along the input-channel dimension (assumed to be dimension 1 for pytorch)</p>
<p>Equivalent to BlockPruningMaskCreator([1, 4]) without restrictions on number
of dimensions, or divisibility</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grouping_fn_name</strong> – The name of the torch grouping function to reduce
dimensions by</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.FourBlockMaskCreator.group_tensor">
<code class="sig-name descname"><span class="pre">group_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#FourBlockMaskCreator.group_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.FourBlockMaskCreator.group_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The mean values of the tensor grouped by blocks of shape
self._block_shape</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">GroupedPruningMaskCreator</span></code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator</span></code></a></p>
<p>Abstract class for a sparsity mask creator that structures masks according to
grouping functions.  Subclasses should implement group_tensor and
_map_mask_to_tensor</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks">
<code class="sig-name descname"><span class="pre">create_sparsity_masks</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_masks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate masks from based on their contained
values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros) can also be a list where each element is a
sparsity for a tensor in the same position in the tensor list. If global
sparsity is enabled, all values of the sparsity list must be the same</p></li>
<li><p><strong>global_sparsity</strong> – if True, sparsity masks will be created such that the
average sparsity across all given tensors is the target sparsity with the
lowest global values masked. If False, each tensor will be masked to the
target sparsity ranking values within each individual tensor. Default is
False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity and all values mapped to the same group have the same
value</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_tensor">
<code class="sig-name descname"><span class="pre">create_sparsity_masks_from_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_masks_from_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensors</strong> – list of tensors to calculate masks based on their values</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks derived from the values of the tensors grouped by
the group_tensor function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_threshold">
<code class="sig-name descname"><span class="pre">create_sparsity_masks_from_threshold</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_masks_from_threshold"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_threshold" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate masks from based on their contained
values</p></li>
<li><p><strong>threshold</strong> – a threshold of group_tensor values to determine cutoff
for sparsification</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks derived from the tensors and the grouped threshold</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor">
<em class="property"><span class="pre">abstract</span> </em><code class="sig-name descname"><span class="pre">group_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.group_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to reduce in groups</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The grouped tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.reduce_tensor">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">reduce_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fn_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.reduce_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.reduce_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to reduce</p></li>
<li><p><strong>dim</strong> – dimension or list of dimension to reduce along</p></li>
<li><p><strong>reduce_fn_name</strong> – function name to reduce tensor with. valid options
are ‘l2’, ‘mean’, ‘max’, ‘min’</p></li>
<li><p><strong>keepdim</strong> – preserves the reduced dimension(s) in returned tensor shape
as shape 1. default is True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor reduced along the given dimension(s)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">PruningMaskCreator</span></code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base abstract class for a sparsity mask creator.
Subclasses should define all methods for creating masks</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks">
<em class="property"><span class="pre">abstract</span> </em><code class="sig-name descname"><span class="pre">create_sparsity_masks</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_masks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate a masks based on their contained
values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros) can also be a list where each element is a
sparsity for a tensor in the same position in the tensor list. If global
sparsity is enabled, all values of the sparsity list must be the same</p></li>
<li><p><strong>global_sparsity</strong> – if True, sparsity masks will be created such that the
average sparsity across all given tensors is the target sparsity with the
lowest global values masked. If False, each tensor will be masked to the
target sparsity ranking values within each individual tensor. Default is
False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_tensor">
<code class="sig-name descname"><span class="pre">create_sparsity_masks_from_tensor</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_masks_from_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_tensor" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensors</strong> – list of tensors to calculate a masks based on their values</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks derived from each of the given tensors</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_threshold">
<em class="property"><span class="pre">abstract</span> </em><code class="sig-name descname"><span class="pre">create_sparsity_masks_from_threshold</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_masks_from_threshold"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_threshold" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate a masks based on their contained
values</p></li>
<li><p><strong>threshold</strong> – a threshold to determine cutoff for sparsification</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks derived from each of the given tensors and the threshold</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">UnstructuredPruningMaskCreator</span></code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</span></code></a></p>
<p>Class for creating unstructured sparsity masks.
Masks will be created using unstructured sparsity by pruning weights ranked
by their value.  Each mask will correspond to the given tensor.</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks">
<code class="sig-name descname"><span class="pre">create_sparsity_masks</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_masks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate a mask from based on their
contained values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros) can also be a list where each element is a
sparsity for a tensor in the same position in the tensor list. If global
sparsity is enabled, all values of the sparsity list must be the same</p></li>
<li><p><strong>global_sparsity</strong> – if True, sparsity masks will be created such that the
average sparsity across all given tensors is the target sparsity with the
lowest global values masked. If False, each tensor will be masked to the
target sparsity ranking values within each individual tensor. Default is
False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity.  If there are more zeros than the desired sparsity,
zeros will be randomly chosen to match the target sparsity</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks_from_threshold">
<code class="sig-name descname"><span class="pre">create_sparsity_masks_from_threshold</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_masks_from_threshold"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks_from_threshold" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – list of tensors to calculate a masks based on their contained
values</p></li>
<li><p><strong>threshold</strong> – a threshold at which to mask values if they are
less than it or equal</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors values &lt;= threshold are masked,
all others are unmasked</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.load_mask_creator">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.</span></code><code class="sig-name descname"><span class="pre">load_mask_creator</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#load_mask_creator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.load_mask_creator" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> – Formatted string or block shape iterable specifying SparsityMaskCreator
object to return</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>SparsityMaskCreator object created from obj</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.mask_pruning">
<span id="sparseml-pytorch-optim-mask-pruning-module"></span><h2>sparseml.pytorch.optim.mask_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.mask_pruning" title="Permalink to this headline"></a></h2>
<p>Code related to applying a mask onto a parameter to impose kernel sparsity,
aka model pruning</p>
<p>NOTE: this file is in the process of being phased out in favor of the
sparsification package. Once all references to mask utils in the optim
package are migrated, this file will be deleted</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_pruning.</span></code><code class="sig-name descname"><span class="pre">ModuleParamPruningMask</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_names</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'weight'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_init</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_unmasked</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_grad_mom</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_creator</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</span></a></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">unstructured</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_names</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'magnitude'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Mask to apply kernel sparsity (model pruning) to a specific parameter in a layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – the layers containing the parameters to mask</p></li>
<li><p><strong>param_names</strong> – the names of the parameter to mask in each layer. If only
one name is given, that name will be applied to all layers that this object
masks. default is weight</p></li>
<li><p><strong>store_init</strong> – store the init weights in a separate variable that can be
used and referenced later</p></li>
<li><p><strong>store_unmasked</strong> – store the unmasked weights in a separate variable that
can be used and referenced later</p></li>
<li><p><strong>track_grad_mom</strong> – store the gradient updates to the parameter with a
momentum variable must be in the range [0.0, 1.0), if set to 0.0 then will
only keep most recent</p></li>
<li><p><strong>mask_creator</strong> – object to define sparisty mask creation,
default is unstructured mask</p></li>
<li><p><strong>layer_names</strong> – the name of the layers the parameters to mask are located in</p></li>
<li><p><strong>global_sparsity</strong> – set True to enable global pruning. if True, when creating
sparsity masks for a target sparsity sparsity masks will be created such that
the average sparsity across all given layers is the target sparsity with the
lowest global values masked. If False, each layer will be masked to the target
sparsity ranking values within each individual tensor. Default is False</p></li>
<li><p><strong>score_type</strong> – the method used to score parameters for masking, i.e.
‘magnitude’, ‘movement’. Default is ‘magnitude’</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.allow_reintroduction">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">allow_reintroduction</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.allow_reintroduction" title="Permalink to this definition"></a></dt>
<dd><p>True if weight reintroduction is allowed</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.apply">
<code class="sig-name descname"><span class="pre">apply</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.apply"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.apply" title="Permalink to this definition"></a></dt>
<dd><p>apply the current mask to the params tensor (zero out the desired values)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param_idx</strong> – index of parameter to apply mask to. if not set, then masks
will be applied to all parameters with available masks</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.disable_reintroduction">
<code class="sig-name descname"><span class="pre">disable_reintroduction</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.disable_reintroduction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.disable_reintroduction" title="Permalink to this definition"></a></dt>
<dd><p>if weight reintroduction is enabled (only during movement pruning),
disables further weight reintroduction</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.enabled">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">enabled</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.enabled" title="Permalink to this definition"></a></dt>
<dd><p>True if the parameter is currently being masked, False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.global_sparsity">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">global_sparsity</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.global_sparsity" title="Permalink to this definition"></a></dt>
<dd><p>True if global pruning is enabled, False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer_names">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">layer_names</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer_names" title="Permalink to this definition"></a></dt>
<dd><p>the names of the layers the parameter to mask is located in</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layers">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">layers</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layers" title="Permalink to this definition"></a></dt>
<dd><p>the layers containing the parameters to mask</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.mask_creator">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">mask_creator</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.mask_creator" title="Permalink to this definition"></a></dt>
<dd><p>SparsityMaskCreator object used to generate masks</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.names">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">names</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.names" title="Permalink to this definition"></a></dt>
<dd><p>the full names of the sparsity masks in the following format:
&lt;LAYER&gt;.&lt;PARAM&gt;.sparsity_mask</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_masks">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">param_masks</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_masks" title="Permalink to this definition"></a></dt>
<dd><p>the current masks applied to each of the parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_names">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">param_names</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_names" title="Permalink to this definition"></a></dt>
<dd><p>the names of the parameters to mask in the layers</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_data">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">params_data</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_data" title="Permalink to this definition"></a></dt>
<dd><p>the current tensors in each of the parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_grad">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">params_grad</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_grad" title="Permalink to this definition"></a></dt>
<dd><p>the current gradient values for each parameter</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_init">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">params_init</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_init" title="Permalink to this definition"></a></dt>
<dd><p>the initial values of the parameters before being masked</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_unmasked">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">params_unmasked</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_unmasked" title="Permalink to this definition"></a></dt>
<dd><p>the unmasked values of the parameters
(stores the last unmasked value before masking)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.pre_optim_step_update">
<code class="sig-name descname"><span class="pre">pre_optim_step_update</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.pre_optim_step_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.pre_optim_step_update" title="Permalink to this definition"></a></dt>
<dd><p>updates scores and buffers that depend on gradients. Should be called
before Optimizer.step() to grab the latest gradients</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.pruning_end">
<code class="sig-name descname"><span class="pre">pruning_end</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">leave_enabled</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.pruning_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.pruning_end" title="Permalink to this definition"></a></dt>
<dd><p>Performs any cleanup necessary for this pruning method.
Disables weight reintroduction if enabled and applies masks</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>leave_enabled</strong> – if False, all pruning hooks will be destroyed. Default
is True</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.reset">
<code class="sig-name descname"><span class="pre">reset</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.reset" title="Permalink to this definition"></a></dt>
<dd><p>resets the current stored tensors such that they will be on the same device
and have the initial data</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.score_type">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">score_type</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.score_type" title="Permalink to this definition"></a></dt>
<dd><p>the scoring method used to create masks (i.e. magnitude, movement)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_data">
<code class="sig-name descname"><span class="pre">set_param_data</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_data" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> – the value to set as the current tensor for the parameter,
if enabled the mask will be applied</p></li>
<li><p><strong>param_idx</strong> – index of the parameter in this object to set the data of</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks">
<code class="sig-name descname"><span class="pre">set_param_masks</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">masks</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>masks</strong> – the masks to set and apply as the current param tensors,
if enabled mask is applied immediately</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_abs_threshold">
<code class="sig-name descname"><span class="pre">set_param_masks_from_abs_threshold</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks_from_abs_threshold"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_abs_threshold" title="Permalink to this definition"></a></dt>
<dd><p>Convenience function to set the parameter masks such that if
abs(value) &lt;= threshold the it a value is masked to 0</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>threshold</strong> – the threshold at which all values will be masked to 0</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_sparsity">
<code class="sig-name descname"><span class="pre">set_param_masks_from_sparsity</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks_from_sparsity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_sparsity" title="Permalink to this definition"></a></dt>
<dd><p>Convenience function to set the parameter masks such that each masks have an
amount of masked values such that the percentage equals the sparsity amount
given. Masks the absolute smallest values up until sparsity is reached.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparsity</strong> – the decimal sparsity to set the param mask to can also be a
list where each element is a sparsity for a tensor in the same position in
the tensor list. If global sparsity is enabled, all values of the sparsity
list must be the same</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_weights">
<code class="sig-name descname"><span class="pre">set_param_masks_from_weights</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks_from_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_weights" title="Permalink to this definition"></a></dt>
<dd><p>Convenience function to set the parameter masks such that the
mask is 1 if a parameter value is non zero and 0 otherwise,
unless otherwise defined by this object’s mask_creator</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_init">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">store_init</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_init" title="Permalink to this definition"></a></dt>
<dd><p>store the init weights in a separate variable that can be used and
referenced later</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_unmasked">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">store_unmasked</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_unmasked" title="Permalink to this definition"></a></dt>
<dd><p>store the unmasked weights in a separate variable that can be used and
referenced later</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.track_grad_mom">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">track_grad_mom</span></code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.track_grad_mom" title="Permalink to this definition"></a></dt>
<dd><p>store the gradient updates to the parameter with a momentum variable
must be in the range [0.0, 1.0), if set to 0.0 then will only
keep most recent</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.mask_pruning_scorer">
<span id="sparseml-pytorch-optim-mask-pruning-scorer-module"></span><h2>sparseml.pytorch.optim.mask_pruning_scorer module<a class="headerlink" href="#module-sparseml.pytorch.optim.mask_pruning_scorer" title="Permalink to this headline"></a></h2>
<p>Classes for tracking and scoring model parameters to generate pruning scores</p>
<p>NOTE: this file is in the process of being phased out in favor of the
sparsification package. Once all references to mask utils in the optim
package are migrated, this file will be deleted</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MagnitudePruningParamsScorer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.</span></code><code class="sig-name descname"><span class="pre">MagnitudePruningParamsScorer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MagnitudePruningParamsScorer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MagnitudePruningParamsScorer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer" title="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer</span></code></a></p>
<p>Scores parameters based on their magnitude</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> – list of model Parameters to track and score</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MagnitudePruningParamsScorer.get_name">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">get_name</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">str</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MagnitudePruningParamsScorer.get_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MagnitudePruningParamsScorer.get_name" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>name of this pruning method</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MagnitudePruningParamsScorer.score_parameters">
<code class="sig-name descname"><span class="pre">score_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MagnitudePruningParamsScorer.score_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MagnitudePruningParamsScorer.score_parameters" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of Tensors the same shapes as the given Parameters where
each Parameter’s elements are scored by their magnitude (absolute value)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.</span></code><code class="sig-name descname"><span class="pre">MovementPruningParamsScorer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MovementPruningParamsScorer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsGradScorer</span></code></p>
<p>Scores parameters based on their movement which is defined as
movement_score = sum(-1.0 * W * dL/dW)</p>
<p>Movement pruning introduced here: <a class="reference external" href="https://arxiv.org/abs/2005.07683">https://arxiv.org/abs/2005.07683</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> – list of model Parameters to track and score</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.check_regen_param_vals">
<code class="sig-name descname"><span class="pre">check_regen_param_vals</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MovementPruningParamsScorer.check_regen_param_vals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.check_regen_param_vals" title="Permalink to this definition"></a></dt>
<dd><p>Check that movement scores are on the correct device and regenerate if not</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.get_name">
<em class="property"><span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">get_name</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">str</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MovementPruningParamsScorer.get_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.get_name" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>name of this pruning method</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.mask_update">
<code class="sig-name descname"><span class="pre">mask_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">masks</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_diffs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MovementPruningParamsScorer.mask_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.mask_update" title="Permalink to this definition"></a></dt>
<dd><p>Resets non main process scores after they have been recorded in the main
process during the mask update</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>masks</strong> – latest masks to be applied to these parameters</p></li>
<li><p><strong>mask_diffs</strong> – mask diff values returned by mask_difference for these
masks that describe how these masks changed since the last update</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.pre_optim_step_update">
<code class="sig-name descname"><span class="pre">pre_optim_step_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">masks</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MovementPruningParamsScorer.pre_optim_step_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.pre_optim_step_update" title="Permalink to this definition"></a></dt>
<dd><p>Update movement scores based on the current Parameter weights and gradients</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>masks</strong> – latest masks that are applied to these parameters</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.score_parameters">
<code class="sig-name descname"><span class="pre">score_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#MovementPruningParamsScorer.score_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.MovementPruningParamsScorer.score_parameters" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of Tensors the same shapes as the given Parameters where
each Parameter’s elements are scored by their weight times the direction
of their gradient.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.</span></code><code class="sig-name descname"><span class="pre">PruningParamsScorer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base abstract class for scoring model parameters for pruning</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> – list of model Parameters to track and score</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.check_regen_param_vals">
<code class="sig-name descname"><span class="pre">check_regen_param_vals</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.check_regen_param_vals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.check_regen_param_vals" title="Permalink to this definition"></a></dt>
<dd><p>Check that all variables based on the params are on the correct device
and regenerate if not</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.get_name">
<em class="property"><span class="pre">abstract</span> <span class="pre">static</span> </em><code class="sig-name descname"><span class="pre">get_name</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">str</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.get_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.get_name" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>name of this pruning method</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.mask_update">
<code class="sig-name descname"><span class="pre">mask_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">masks</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_diffs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.mask_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.mask_update" title="Permalink to this definition"></a></dt>
<dd><p>Perform any updates based on the latest mask to be applied to the weights
immediately after this function completes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>masks</strong> – latest masks to be applied to these parameters</p></li>
<li><p><strong>mask_diffs</strong> – mask diff values returned by mask_difference for these
masks that describe how these masks changed since the last update</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.on_pruning_end">
<code class="sig-name descname"><span class="pre">on_pruning_end</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.on_pruning_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.on_pruning_end" title="Permalink to this definition"></a></dt>
<dd><p>Perform any cleanup after pruning is complete</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.pre_optim_step_update">
<code class="sig-name descname"><span class="pre">pre_optim_step_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">masks</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.pre_optim_step_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.pre_optim_step_update" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Perform any required logic for tracking Parameter data and gradients before</dt><dd><p>an Optimizer step is applied to the model.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>masks</strong> – latest masks that are applied to these parameters</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.score_parameters">
<em class="property"><span class="pre">abstract</span> </em><code class="sig-name descname"><span class="pre">score_parameters</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.score_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.score_parameters" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of Tensors the same shapes as the given Parameters that
correspond to their scores to be pruned by</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.update_last_applied_sparsity">
<code class="sig-name descname"><span class="pre">update_last_applied_sparsity</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#PruningParamsScorer.update_last_applied_sparsity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer.update_last_applied_sparsity" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparsity</strong> – sparsity level between 0.0 and 1.0 that was the last value
set for the given parameters</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.mask_pruning_scorer.create_pruning_param_scorer">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.</span></code><code class="sig-name descname"><span class="pre">create_pruning_param_scorer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer" title="sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer"><span class="pre">sparseml.pytorch.optim.mask_pruning_scorer.PruningParamsScorer</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning_scorer.html#create_pruning_param_scorer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning_scorer.create_pruning_param_scorer" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – List of Parameters for the created PruningParamsScorer to track</p></li>
<li><p><strong>score_type</strong> – String name of scoring type to use. Valid options are
‘magnitude’, or ‘movement’</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="sparseml-pytorch-optim-modifier-module">
<h2>sparseml.pytorch.optim.modifier module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-as-module">
<h2>sparseml.pytorch.optim.modifier_as module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-as-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-distillation-module">
<h2>sparseml.pytorch.optim.modifier_distillation module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-distillation-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-epoch-module">
<h2>sparseml.pytorch.optim.modifier_epoch module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-epoch-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-lr-module">
<h2>sparseml.pytorch.optim.modifier_lr module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-lr-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-params-module">
<h2>sparseml.pytorch.optim.modifier_params module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-params-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-pruning-module">
<h2>sparseml.pytorch.optim.modifier_pruning module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-pruning-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-quantization-module">
<h2>sparseml.pytorch.optim.modifier_quantization module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-quantization-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-regularizer-module">
<h2>sparseml.pytorch.optim.modifier_regularizer module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-regularizer-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="sparseml-pytorch-optim-modifier-thinning-module">
<h2>sparseml.pytorch.optim.modifier_thinning module<a class="headerlink" href="#sparseml-pytorch-optim-modifier-thinning-module" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="module-sparseml.pytorch.optim.optimizer">
<span id="sparseml-pytorch-optim-optimizer-module"></span><h2>sparseml.pytorch.optim.optimizer module<a class="headerlink" href="#module-sparseml.pytorch.optim.optimizer" title="Permalink to this headline"></a></h2>
<p>Optimizer wrapper for enforcing Modifiers on the training process of a Module.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.optimizer.</span></code><code class="sig-name descname"><span class="pre">ScheduledOptimizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">manager</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="sparseml.pytorch.optim.manager.ScheduledModifierManager"><span class="pre">sparseml.pytorch.optim.manager.ScheduledModifierManager</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger"><span class="pre">sparseml.pytorch.utils.logger.BaseLogger</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initialize_kwargs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>An optimizer wrapper to handle applying modifiers according to their schedule
to both the passed in optimizer and the module.</p>
<p>Overrides the step() function so that this method can call before and after on the
modifiers to apply appropriate modifications to both the optimizer and the module.</p>
<p>The epoch_start and epoch_end are based on how many steps have been taken
along with the steps_per_epoch.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- training cycle</div>
<div class="line-block">
<div class="line">- zero_grad</div>
<div class="line">- loss_update</div>
<div class="line-block">
<div class="line">- modifiers.loss_update</div>
</div>
<div class="line">- step</div>
<div class="line-block">
<div class="line">- modifiers.update</div>
<div class="line">- modifiers.optimizer_pre_step</div>
<div class="line">- optimizer.step</div>
<div class="line">- modifiers.optimizers_post_step</div>
</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>manager</strong> – the manager or list of managers used to apply modifications</p></li>
<li><p><strong>steps_per_epoch</strong> – the number of steps or batches in each epoch,
not strictly required and can be set to -1.
used to calculate decimals within the epoch,
when not using can result in irregularities</p></li>
<li><p><strong>loggers</strong> – logger manager to log important info to within the modifiers;
ex tensorboard or to the console</p></li>
<li><p><strong>initialize_kwargs</strong> – key word arguments and values to be passed to
the recipe manager initialize function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.adjust_current_step">
<code class="sig-name descname"><span class="pre">adjust_current_step</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.adjust_current_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.adjust_current_step" title="Permalink to this definition"></a></dt>
<dd><p>Adjust the current step for the manager’s schedule to the given epoch and step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – the epoch to set the current global step to match</p></li>
<li><p><strong>step</strong> – the step (batch) within the epoch to set the
current global step to match</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.learning_rate">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">learning_rate</span></code><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.learning_rate" title="Permalink to this definition"></a></dt>
<dd><p>convenience function to get the first learning rate for any of
the param groups in the optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_manager_state_dict">
<code class="sig-name descname"><span class="pre">load_manager_state_dict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.load_manager_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_manager_state_dict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.loss_update">
<code class="sig-name descname"><span class="pre">loss_update</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.Tensor</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.loss_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.loss_update" title="Permalink to this definition"></a></dt>
<dd><p>Optional call to update modifiers based on the calculated loss.
Not needed unless one or more of the modifier is using the loss
to make a modification or is modifying the loss itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> – the calculated loss after running a forward pass and loss_fn</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">manager</span></code><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager" title="Permalink to this definition"></a></dt>
<dd><p>The ScheduledModifierManager for this optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager_state_dict">
<code class="sig-name descname"><span class="pre">manager_state_dict</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.manager_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager_state_dict" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.step">
<code class="sig-name descname"><span class="pre">step</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.step" title="Permalink to this definition"></a></dt>
<dd><p>Called to perform a step on the optimizer activation normal.
Updates the current epoch based on the step count.
Calls into modifiers before the step happens.
Calls into modifiers after the step happens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> – optional closure passed into the contained optimizer
for the step</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.sensitivity_as">
<span id="sparseml-pytorch-optim-sensitivity-as-module"></span><h2>sparseml.pytorch.optim.sensitivity_as module<a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_as" title="Permalink to this headline"></a></h2>
<p>Sensitivity analysis implementations for increasing activation sparsity by using FATReLU</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_as.</span></code><code class="sig-name descname"><span class="pre">ASLayerTracker</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_input</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_func</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_func</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An implementation for tracking activation sparsity properties for a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> – the module to track activation sparsity for</p></li>
<li><p><strong>track_input</strong> – track the input sparsity for the module</p></li>
<li><p><strong>track_output</strong> – track the output sparsity for the module</p></li>
<li><p><strong>input_func</strong> – the function to call on input to the layer
and receives the input tensor</p></li>
<li><p><strong>output_func</strong> – the function to call on output to the layer
and receives the output tensor</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.clear">
<code class="sig-name descname"><span class="pre">clear</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clear out current results for the model</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.disable">
<code class="sig-name descname"><span class="pre">disable</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.disable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.disable" title="Permalink to this definition"></a></dt>
<dd><p>Disable the forward hooks for the layer</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.enable">
<code class="sig-name descname"><span class="pre">enable</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.enable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.enable" title="Permalink to this definition"></a></dt>
<dd><p>Enable the forward hooks to the layer</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_input">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">tracked_input</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_input" title="Permalink to this definition"></a></dt>
<dd><p>the current tracked input results</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_output">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">tracked_output</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_output" title="Permalink to this definition"></a></dt>
<dd><p>the current tracked output results</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_as.</span></code><code class="sig-name descname"><span class="pre">LayerBoostResults</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boosted_as</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boosted_loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunResults" title="sparseml.pytorch.utils.module.ModuleRunResults"><span class="pre">sparseml.pytorch.utils.module.ModuleRunResults</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">baseline_as</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">baseline_loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunResults" title="sparseml.pytorch.utils.module.ModuleRunResults"><span class="pre">sparseml.pytorch.utils.module.ModuleRunResults</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#LayerBoostResults"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Results for a specific threshold set in a FATReLU layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the name of the layer the results are for</p></li>
<li><p><strong>threshold</strong> – the threshold used in the FATReLU layer</p></li>
<li><p><strong>boosted_as</strong> – the measured activation sparsity after threshold is applied</p></li>
<li><p><strong>boosted_loss</strong> – the measured loss after threshold is applied</p></li>
<li><p><strong>baseline_as</strong> – the measured activation sparsity before threshold is applied</p></li>
<li><p><strong>baseline_loss</strong> – the measured loss before threshold is applied</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_as">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">baseline_as</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_as" title="Permalink to this definition"></a></dt>
<dd><p>the measured activation sparsity before threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_loss">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">baseline_loss</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_loss" title="Permalink to this definition"></a></dt>
<dd><p>the measured loss before threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_as">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">boosted_as</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_as" title="Permalink to this definition"></a></dt>
<dd><p>the measured activation sparsity after threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_loss">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">boosted_loss</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_loss" title="Permalink to this definition"></a></dt>
<dd><p>the measured loss after threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.name">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">name</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.name" title="Permalink to this definition"></a></dt>
<dd><p>the name of the layer the results are for</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.threshold">
<em class="property"><span class="pre">property</span> </em><code class="sig-name descname"><span class="pre">threshold</span></code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.threshold" title="Permalink to this definition"></a></dt>
<dd><p>the threshold used in the FATReLU layer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_as.</span></code><code class="sig-name descname"><span class="pre">ModuleASOneShootBooster</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.utils.data.dataset.Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader_kwargs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ModuleASOneShootBooster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implementation class for boosting the activation sparsity in a given module
using FATReLUs.
Programmatically goes through and figures out the best thresholds to limit loss
based on provided parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to boost</p></li>
<li><p><strong>device</strong> – the device to run the analysis on; ex [cpu, cuda, cuda:1]</p></li>
<li><p><strong>dataset</strong> – the dataset used to evaluate the boosting on</p></li>
<li><p><strong>batch_size</strong> – the batch size to run through the module in test mode</p></li>
<li><p><strong>loss</strong> – the loss function to use for calculations</p></li>
<li><p><strong>data_loader_kwargs</strong> – any keyword arguments to supply to a the
DataLoader constructor</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster.run_layers">
<code class="sig-name descname"><span class="pre">run_layers</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_target_metric_loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_key</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_increases</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><a class="reference internal" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults" title="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults"><span class="pre">sparseml.pytorch.optim.sensitivity_as.LayerBoostResults</span></a><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ModuleASOneShootBooster.run_layers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster.run_layers" title="Permalink to this definition"></a></dt>
<dd><p>Run the booster for the specified layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – names of the layers to run boosting on</p></li>
<li><p><strong>max_target_metric_loss</strong> – the max loss in the target metric that
can happen while boosting</p></li>
<li><p><strong>metric_key</strong> – the name of the metric to evaluate while boosting;
ex: [__loss__, top1acc, top5acc]. Must exist in the LossWrapper</p></li>
<li><p><strong>metric_increases</strong> – True if the metric increases for worse loss such as in
a CrossEntropyLoss, False if the metric decreases for worse such as in
accuracy</p></li>
<li><p><strong>precision</strong> – the precision to check the results to. Larger values here will
give less precise results but won’t take as long</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The results for the boosting</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.sensitivity_lr">
<span id="sparseml-pytorch-optim-sensitivity-lr-module"></span><h2>sparseml.pytorch.optim.sensitivity_lr module<a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_lr" title="Permalink to this headline"></a></h2>
<p>Sensitivity analysis implementations for learning rate on Modules against loss funcs.</p>
<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_lr.default_exponential_check_lrs">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_lr.</span></code><code class="sig-name descname"><span class="pre">default_exponential_check_lrs</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_lr</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_lr</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_mult</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.1</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_lr.html#default_exponential_check_lrs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_lr.default_exponential_check_lrs" title="Permalink to this definition"></a></dt>
<dd><p>Get the default learning rates to check between init_lr and final_lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_lr</strong> – the initial learning rate in the returned list</p></li>
<li><p><strong>final_lr</strong> – the final learning rate in the returned list</p></li>
<li><p><strong>lr_mult</strong> – the multiplier increase for each step between
init_lr and final_lr</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the list of created lrs that increase exponentially between
init_lr and final_lr according to lr_mult</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_lr.lr_loss_sensitivity">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_lr.</span></code><code class="sig-name descname"><span class="pre">lr_loss_sensitivity</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.utils.data.dataloader.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_measurement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_lrs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">(1e-06,</span> <span class="pre">1.1e-06,</span> <span class="pre">1.21e-06,</span> <span class="pre">1.3310000000000003e-06,</span> <span class="pre">1.4641000000000003e-06,</span> <span class="pre">1.6105100000000006e-06,</span> <span class="pre">1.7715610000000007e-06,</span> <span class="pre">1.948717100000001e-06,</span> <span class="pre">2.1435888100000012e-06,</span> <span class="pre">2.3579476910000015e-06,</span> <span class="pre">2.5937424601000017e-06,</span> <span class="pre">2.853116706110002e-06,</span> <span class="pre">3.1384283767210024e-06,</span> <span class="pre">3.452271214393103e-06,</span> <span class="pre">3.7974983358324136e-06,</span> <span class="pre">4.177248169415655e-06,</span> <span class="pre">4.594972986357221e-06,</span> <span class="pre">5.0544702849929435e-06,</span> <span class="pre">5.559917313492238e-06,</span> <span class="pre">6.115909044841462e-06,</span> <span class="pre">6.727499949325609e-06,</span> <span class="pre">7.40024994425817e-06,</span> <span class="pre">8.140274938683989e-06,</span> <span class="pre">8.954302432552388e-06,</span> <span class="pre">9.849732675807628e-06,</span> <span class="pre">1.0834705943388392e-05,</span> <span class="pre">1.1918176537727232e-05,</span> <span class="pre">1.3109994191499957e-05,</span> <span class="pre">1.4420993610649954e-05,</span> <span class="pre">1.586309297171495e-05,</span> <span class="pre">1.7449402268886447e-05,</span> <span class="pre">1.9194342495775094e-05,</span> <span class="pre">2.1113776745352607e-05,</span> <span class="pre">2.322515441988787e-05,</span> <span class="pre">2.554766986187666e-05,</span> <span class="pre">2.8102436848064327e-05,</span> <span class="pre">3.091268053287076e-05,</span> <span class="pre">3.4003948586157844e-05,</span> <span class="pre">3.7404343444773634e-05,</span> <span class="pre">4.1144777789251e-05,</span> <span class="pre">4.52592555681761e-05,</span> <span class="pre">4.978518112499371e-05,</span> <span class="pre">5.4763699237493086e-05,</span> <span class="pre">6.02400691612424e-05,</span> <span class="pre">6.626407607736664e-05,</span> <span class="pre">7.289048368510331e-05,</span> <span class="pre">8.017953205361364e-05,</span> <span class="pre">8.819748525897502e-05,</span> <span class="pre">9.701723378487253e-05,</span> <span class="pre">0.00010671895716335979,</span> <span class="pre">0.00011739085287969578,</span> <span class="pre">0.00012912993816766537,</span> <span class="pre">0.00014204293198443192,</span> <span class="pre">0.00015624722518287512,</span> <span class="pre">0.00017187194770116264,</span> <span class="pre">0.00018905914247127894,</span> <span class="pre">0.00020796505671840686,</span> <span class="pre">0.00022876156239024756,</span> <span class="pre">0.00025163771862927233,</span> <span class="pre">0.0002768014904921996,</span> <span class="pre">0.0003044816395414196,</span> <span class="pre">0.00033492980349556157,</span> <span class="pre">0.00036842278384511775,</span> <span class="pre">0.0004052650622296296,</span> <span class="pre">0.0004457915684525926,</span> <span class="pre">0.0004903707252978519,</span> <span class="pre">0.0005394077978276372,</span> <span class="pre">0.000593348577610401,</span> <span class="pre">0.0006526834353714411,</span> <span class="pre">0.0007179517789085853,</span> <span class="pre">0.0007897469567994438,</span> <span class="pre">0.0008687216524793883,</span> <span class="pre">0.0009555938177273272,</span> <span class="pre">0.00105115319950006,</span> <span class="pre">0.001156268519450066,</span> <span class="pre">0.0012718953713950728,</span> <span class="pre">0.0013990849085345801,</span> <span class="pre">0.0015389933993880383,</span> <span class="pre">0.0016928927393268422,</span> <span class="pre">0.0018621820132595267,</span> <span class="pre">0.0020484002145854797,</span> <span class="pre">0.0022532402360440277,</span> <span class="pre">0.0024785642596484307,</span> <span class="pre">0.002726420685613274,</span> <span class="pre">0.0029990627541746015,</span> <span class="pre">0.003298969029592062,</span> <span class="pre">0.0036288659325512686,</span> <span class="pre">0.003991752525806396,</span> <span class="pre">0.0043909277783870364,</span> <span class="pre">0.004830020556225741,</span> <span class="pre">0.005313022611848316,</span> <span class="pre">0.005844324873033148,</span> <span class="pre">0.006428757360336463,</span> <span class="pre">0.00707163309637011,</span> <span class="pre">0.007778796406007121,</span> <span class="pre">0.008556676046607835,</span> <span class="pre">0.009412343651268619,</span> <span class="pre">0.010353578016395481,</span> <span class="pre">0.01138893581803503,</span> <span class="pre">0.012527829399838533,</span> <span class="pre">0.013780612339822387,</span> <span class="pre">0.015158673573804626,</span> <span class="pre">0.01667454093118509,</span> <span class="pre">0.0183419950243036,</span> <span class="pre">0.020176194526733963,</span> <span class="pre">0.02219381397940736,</span> <span class="pre">0.0244131953773481,</span> <span class="pre">0.02685451491508291,</span> <span class="pre">0.029539966406591206,</span> <span class="pre">0.03249396304725033,</span> <span class="pre">0.03574335935197537,</span> <span class="pre">0.03931769528717291,</span> <span class="pre">0.043249464815890204,</span> <span class="pre">0.047574411297479226,</span> <span class="pre">0.052331852427227155,</span> <span class="pre">0.05756503766994987,</span> <span class="pre">0.06332154143694486,</span> <span class="pre">0.06965369558063936,</span> <span class="pre">0.0766190651387033,</span> <span class="pre">0.08428097165257363,</span> <span class="pre">0.092709068817831,</span> <span class="pre">0.10197997569961412,</span> <span class="pre">0.11217797326957554,</span> <span class="pre">0.1233957705965331,</span> <span class="pre">0.13573534765618642,</span> <span class="pre">0.14930888242180507,</span> <span class="pre">0.1642397706639856,</span> <span class="pre">0.18066374773038418,</span> <span class="pre">0.19873012250342262,</span> <span class="pre">0.2186031347537649,</span> <span class="pre">0.2404634482291414,</span> <span class="pre">0.2645097930520556,</span> <span class="pre">0.29096077235726114,</span> <span class="pre">0.3200568495929873,</span> <span class="pre">0.3520625345522861,</span> <span class="pre">0.38726878800751474,</span> <span class="pre">0.4259956668082662,</span> <span class="pre">0.4685952334890929,</span> <span class="pre">0.5154547568380022,</span> <span class="pre">0.5)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_key</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'__loss__'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer_run_funcs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunFuncs" title="sparseml.pytorch.utils.module.ModuleRunFuncs"><span class="pre">sparseml.pytorch.utils.module.ModuleRunFuncs</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer_loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger"><span class="pre">sparseml.pytorch.utils.logger.BaseLogger</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.LRLossSensitivityAnalysis" title="sparseml.optim.sensitivity.LRLossSensitivityAnalysis"><span class="pre">sparseml.optim.sensitivity.LRLossSensitivityAnalysis</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_lr.html#lr_loss_sensitivity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_lr.lr_loss_sensitivity" title="Permalink to this definition"></a></dt>
<dd><p>Implementation for handling running sensitivity analysis for
learning rates on modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to run the learning rate sensitivity analysis over,
it is expected to already be on the correct device</p></li>
<li><p><strong>data</strong> – the data to run through the module for calculating
the sensitivity analysis</p></li>
<li><p><strong>loss</strong> – the loss function to use for the sensitivity analysis</p></li>
<li><p><strong>optim</strong> – the optimizer to run the sensitivity analysis with</p></li>
<li><p><strong>device</strong> – the device to run the analysis on; ex: cpu, cuda.
module must already be on that device, this is used to place then data
on that same device.</p></li>
<li><p><strong>steps_per_measurement</strong> – the number of batches to run through for
the analysis at each LR</p></li>
<li><p><strong>check_lrs</strong> – the learning rates to check for analysis
(will sort them small to large before running)</p></li>
<li><p><strong>loss_key</strong> – the key for the loss function to track in the returned dict</p></li>
<li><p><strong>trainer_run_funcs</strong> – override functions for ModuleTrainer class</p></li>
<li><p><strong>trainer_loggers</strong> – loggers to log data to while running the analysis</p></li>
<li><p><strong>show_progress</strong> – track progress of the runs if True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of tuples containing the analyzed learning rate at 0
and the ModuleRunResults in 1, ModuleRunResults being a collection
of all the batch results run through the module at that LR</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.sensitivity_pruning">
<span id="sparseml-pytorch-optim-sensitivity-pruning-module"></span><h2>sparseml.pytorch.optim.sensitivity_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_pruning" title="Permalink to this headline"></a></h2>
<p>Sensitivity analysis implementations for kernel sparsity on Modules against loss funcs.</p>
<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_pruning.model_prunability_magnitude">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_pruning.</span></code><code class="sig-name descname"><span class="pre">model_prunability_magnitude</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#model_prunability_magnitude"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.model_prunability_magnitude" title="Permalink to this definition"></a></dt>
<dd><p>Calculate the approximate sensitivity for an overall model.
Range of the values are not scaled to anything, so must be taken in context
with other known models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the model to calculate the sensitivity for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the approximated sensitivity</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_magnitude">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_pruning.</span></code><code class="sig-name descname"><span class="pre">pruning_loss_sens_magnitude</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity_levels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="p"><span class="pre">…</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">(0.0,</span> <span class="pre">0.01,</span> <span class="pre">0.02,</span> <span class="pre">0.03,</span> <span class="pre">0.04,</span> <span class="pre">0.05,</span> <span class="pre">0.06,</span> <span class="pre">0.07,</span> <span class="pre">0.08,</span> <span class="pre">0.09,</span> <span class="pre">0.1,</span> <span class="pre">0.11,</span> <span class="pre">0.12,</span> <span class="pre">0.13,</span> <span class="pre">0.14,</span> <span class="pre">0.15,</span> <span class="pre">0.16,</span> <span class="pre">0.17,</span> <span class="pre">0.18,</span> <span class="pre">0.19,</span> <span class="pre">0.2,</span> <span class="pre">0.21,</span> <span class="pre">0.22,</span> <span class="pre">0.23,</span> <span class="pre">0.24,</span> <span class="pre">0.25,</span> <span class="pre">0.26,</span> <span class="pre">0.27,</span> <span class="pre">0.28,</span> <span class="pre">0.29,</span> <span class="pre">0.3,</span> <span class="pre">0.31,</span> <span class="pre">0.32,</span> <span class="pre">0.33,</span> <span class="pre">0.34,</span> <span class="pre">0.35,</span> <span class="pre">0.36,</span> <span class="pre">0.37,</span> <span class="pre">0.38,</span> <span class="pre">0.39,</span> <span class="pre">0.4,</span> <span class="pre">0.41,</span> <span class="pre">0.42,</span> <span class="pre">0.43,</span> <span class="pre">0.44,</span> <span class="pre">0.45,</span> <span class="pre">0.46,</span> <span class="pre">0.47,</span> <span class="pre">0.48,</span> <span class="pre">0.49,</span> <span class="pre">0.5,</span> <span class="pre">0.51,</span> <span class="pre">0.52,</span> <span class="pre">0.53,</span> <span class="pre">0.54,</span> <span class="pre">0.55,</span> <span class="pre">0.56,</span> <span class="pre">0.57,</span> <span class="pre">0.58,</span> <span class="pre">0.59,</span> <span class="pre">0.6,</span> <span class="pre">0.61,</span> <span class="pre">0.62,</span> <span class="pre">0.63,</span> <span class="pre">0.64,</span> <span class="pre">0.65,</span> <span class="pre">0.66,</span> <span class="pre">0.67,</span> <span class="pre">0.68,</span> <span class="pre">0.69,</span> <span class="pre">0.7,</span> <span class="pre">0.71,</span> <span class="pre">0.72,</span> <span class="pre">0.73,</span> <span class="pre">0.74,</span> <span class="pre">0.75,</span> <span class="pre">0.76,</span> <span class="pre">0.77,</span> <span class="pre">0.78,</span> <span class="pre">0.79,</span> <span class="pre">0.8,</span> <span class="pre">0.81,</span> <span class="pre">0.82,</span> <span class="pre">0.83,</span> <span class="pre">0.84,</span> <span class="pre">0.85,</span> <span class="pre">0.86,</span> <span class="pre">0.87,</span> <span class="pre">0.88,</span> <span class="pre">0.89,</span> <span class="pre">0.9,</span> <span class="pre">0.91,</span> <span class="pre">0.92,</span> <span class="pre">0.93,</span> <span class="pre">0.94,</span> <span class="pre">0.95,</span> <span class="pre">0.96,</span> <span class="pre">0.97,</span> <span class="pre">0.98,</span> <span class="pre">0.99)</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis"><span class="pre">sparseml.optim.sensitivity.PruningLossSensitivityAnalysis</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#pruning_loss_sens_magnitude"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_magnitude" title="Permalink to this definition"></a></dt>
<dd><p>Approximated kernel sparsity (pruning) loss analysis for a given model.
Returns the results for each prunable param (conv, linear) in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the model to calculate the sparse sensitivity analysis for</p></li>
<li><p><strong>sparsity_levels</strong> – the sparsity levels to calculate the loss for for each param</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the analysis results for the model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_one_shot">
<code class="sig-prename descclassname"><span class="pre">sparseml.pytorch.optim.sensitivity_pruning.</span></code><code class="sig-name descname"><span class="pre">pruning_loss_sens_one_shot</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.utils.data.dataloader.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></a><span class="p"><span class="pre">,</span> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps_per_measurement</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity_levels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">(0.0,</span> <span class="pre">0.2,</span> <span class="pre">0.4,</span> <span class="pre">0.6,</span> <span class="pre">0.7,</span> <span class="pre">0.8,</span> <span class="pre">0.85,</span> <span class="pre">0.9,</span> <span class="pre">0.95,</span> <span class="pre">0.99)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_key</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'__loss__'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tester_run_funcs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunFuncs" title="sparseml.pytorch.utils.module.ModuleRunFuncs"><span class="pre">sparseml.pytorch.utils.module.ModuleRunFuncs</span></a><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tester_loggers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger"><span class="pre">sparseml.pytorch.utils.logger.BaseLogger</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis"><span class="pre">sparseml.optim.sensitivity.PruningLossSensitivityAnalysis</span></a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#pruning_loss_sens_one_shot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_one_shot" title="Permalink to this definition"></a></dt>
<dd><p>Run a one shot sensitivity analysis for kernel sparsity.
It does not retrain, and instead puts the model to eval mode.
Moves layer by layer to calculate the sensitivity analysis for each and
resets the previously run layers.
Note, by default it caches the data.
This means it is not parallel for data loading and the first run can take longer.
Subsequent sparsity checks for layers and levels will be much faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to run the kernel sparsity sensitivity analysis over
will extract all prunable layers out</p></li>
<li><p><strong>data</strong> – the data to run through the module for calculating the sensitivity
analysis</p></li>
<li><p><strong>loss</strong> – the loss function to use for the sensitivity analysis</p></li>
<li><p><strong>device</strong> – the device to run the analysis on; ex: cpu, cuda</p></li>
<li><p><strong>steps_per_measurement</strong> – the number of samples or items to take for each
measurement at each sparsity lev</p></li>
<li><p><strong>sparsity_levels</strong> – the sparsity levels to check for each layer to calculate
sensitivity</p></li>
<li><p><strong>loss_key</strong> – the key for the loss function to track in the returned dict</p></li>
<li><p><strong>tester_run_funcs</strong> – override functions to use in the ModuleTester that runs</p></li>
<li><p><strong>tester_loggers</strong> – loggers to log data to while running the analysis</p></li>
<li><p><strong>show_progress</strong> – track progress of the runs if True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sensitivity results for every layer that is prunable</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sparseml.pytorch.optim" title="Permalink to this headline"></a></h2>
<p>Recalibration code for the PyTorch framework.
Handles things like model pruning and increasing activation sparsity.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sparseml.pytorch.nn.html" class="btn btn-neutral float-left" title="sparseml.pytorch.nn package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sparseml.pytorch.sparsification.html" class="btn btn-neutral float-right" title="sparseml.pytorch.sparsification package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v0.11.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../v0.10.0/api/sparseml.pytorch.optim.html">v0.10.0</a></dd>
      <dd><a href="../../v0.10.1/api/sparseml.pytorch.optim.html">v0.10.1</a></dd>
      <dd><a href="sparseml.pytorch.optim.html">v0.11.0</a></dd>
      <dd><a href="../../v0.11.1/api/sparseml.pytorch.optim.html">v0.11.1</a></dd>
      <dd><a href="../../v0.12/api/sparseml.pytorch.optim.html">v0.12</a></dd>
      <dd><a href="../../v0.12.0/api/sparseml.pytorch.optim.html">v0.12.0</a></dd>
      <dd><a href="../../v0.12.1/api/sparseml.pytorch.optim.html">v0.12.1</a></dd>
      <dd><a href="../../v0.12.2/api/sparseml.pytorch.optim.html">v0.12.2</a></dd>
      <dd><a href="../../v0.3.0/api/sparseml.pytorch.optim.html">v0.3.0</a></dd>
      <dd><a href="../../v0.3.1/api/sparseml.pytorch.optim.html">v0.3.1</a></dd>
      <dd><a href="../../v0.4.0/api/sparseml.pytorch.optim.html">v0.4.0</a></dd>
      <dd><a href="../../v0.5.0/api/sparseml.pytorch.optim.html">v0.5.0</a></dd>
      <dd><a href="../../v0.5.1/api/sparseml.pytorch.optim.html">v0.5.1</a></dd>
      <dd><a href="../../v0.6.0/api/sparseml.pytorch.optim.html">v0.6.0</a></dd>
      <dd><a href="../../v0.7.0/api/sparseml.pytorch.optim.html">v0.7.0</a></dd>
      <dd><a href="../../v0.8.0/api/sparseml.pytorch.optim.html">v0.8.0</a></dd>
      <dd><a href="../../v0.9.0/api/sparseml.pytorch.optim.html">v0.9.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../../main/api/sparseml.pytorch.optim.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-128364174-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-128364174-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>