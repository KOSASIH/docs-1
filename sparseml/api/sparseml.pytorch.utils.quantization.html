<!DOCTYPE html>
<html class="writer-html5" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   sparseml.pytorch.utils.quantization package — SparseML 0.7.0.20211006 documentation
  </title>
  <link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/css/nm-theme-adjustment.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/favicon.ico" rel="shortcut icon"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/clipboard.min.js">
  </script>
  <script src="../_static/copybutton.js">
  </script>
  <script src="../_static/js/theme.js">
  </script>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="sparseml.sparsification.html" rel="next" title="sparseml.sparsification package"/>
  <link href="sparseml.pytorch.utils.html" rel="prev" title="sparseml.pytorch.utils package"/>
 </head>
 <body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
   <nav class="wy-nav-side" data-toggle="wy-nav-shift">
    <div class="wy-side-scroll">
     <div class="wy-side-nav-search">
      <a class="icon icon-home" href="../index.html">
       SparseML
       <img alt="Logo" class="logo" src="../_static/icon-sparseml.png"/>
      </a>
      <div class="version">
       0.7
      </div>
      <div role="search">
       <form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
        <input name="q" placeholder="Search docs" type="text"/>
        <input name="check_keywords" type="hidden" value="yes"/>
        <input name="area" type="hidden" value="default"/>
       </form>
      </div>
     </div>
     <div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
      <p class="caption">
       <span class="caption-text">
        General
       </span>
      </p>
      <ul>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/installation.html">
         Installation
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/code.html">
         Sparsification Code
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/recipes.html">
         Sparsification Recipes
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/onnx_export.html">
         ONNX Export
        </a>
       </li>
      </ul>
      <p class="caption">
       <span class="caption-text">
        API
       </span>
      </p>
      <ul class="current">
       <li class="toctree-l1 current">
        <a class="reference internal" href="sparseml.html">
         sparseml package
        </a>
        <ul class="current">
         <li class="toctree-l2 current">
          <a class="reference internal" href="sparseml.html#subpackages">
           Subpackages
          </a>
          <ul class="current">
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.deepsparse.html">
             sparseml.deepsparse package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.framework.html">
             sparseml.framework package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.keras.html">
             sparseml.keras package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.onnx.html">
             sparseml.onnx package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.optim.html">
             sparseml.optim package
            </a>
           </li>
           <li class="toctree-l3 current">
            <a class="reference internal" href="sparseml.pytorch.html">
             sparseml.pytorch package
            </a>
            <ul class="current">
             <li class="toctree-l4 current">
              <a class="reference internal" href="sparseml.pytorch.html#subpackages">
               Subpackages
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#submodules">
               Submodules
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch.base">
               sparseml.pytorch.base module
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch">
               Module contents
              </a>
             </li>
            </ul>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.sparsification.html">
             sparseml.sparsification package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.tensorflow_v1.html">
             sparseml.tensorflow_v1 package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.utils.html">
             sparseml.utils package
            </a>
           </li>
          </ul>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#submodules">
           Submodules
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.base">
           sparseml.base module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.log">
           sparseml.log module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.version">
           sparseml.version module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml">
           Module contents
          </a>
         </li>
        </ul>
       </li>
      </ul>
      <p class="caption">
       <span class="caption-text">
        Connect Online
       </span>
      </p>
      <ul>
       <li class="toctree-l1">
        <a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">
         Bugs, Feature Requests
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://discuss.neuralmagic.com/">
         Support, General Q&amp;A Forums
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ">
         Deep Sparse Community Slack
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://github.com/neuralmagic">
         Neural Magic GitHub
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://docs.neuralmagic.com">
         Neural Magic Docs
        </a>
       </li>
      </ul>
     </div>
    </div>
   </nav>
   <section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
    <nav aria-label="Mobile navigation menu" class="wy-nav-top">
     <i class="fa fa-bars" data-toggle="wy-nav-top">
     </i>
     <a href="../index.html">
      SparseML
     </a>
    </nav>
    <div class="wy-nav-content">
     <div class="rst-content">
      <div aria-label="Page navigation" role="navigation">
       <ul class="wy-breadcrumbs">
        <li>
         <a class="icon icon-home" href="../index.html">
         </a>
         »
        </li>
        <li>
         <a href="sparseml.html">
          sparseml package
         </a>
         »
        </li>
        <li>
         <a href="sparseml.pytorch.html">
          sparseml.pytorch package
         </a>
         »
        </li>
        <li>
         <a href="sparseml.pytorch.utils.html">
          sparseml.pytorch.utils package
         </a>
         »
        </li>
        <li>
         sparseml.pytorch.utils.quantization package
        </li>
        <li class="wy-breadcrumbs-aside">
         <a href="../_sources/api/sparseml.pytorch.utils.quantization.rst.txt" rel="nofollow">
          View page source
         </a>
        </li>
       </ul>
       <hr/>
      </div>
      <div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <div itemprop="articleBody">
        <div class="section" id="sparseml-pytorch-utils-quantization-package">
         <h1>
          sparseml.pytorch.utils.quantization package
          <a class="headerlink" href="#sparseml-pytorch-utils-quantization-package" title="Permalink to this headline">
           
          </a>
         </h1>
         <div class="section" id="submodules">
          <h2>
           Submodules
           <a class="headerlink" href="#submodules" title="Permalink to this headline">
            
           </a>
          </h2>
         </div>
         <div class="section" id="module-sparseml.pytorch.utils.quantization.helpers">
          <span id="sparseml-pytorch-utils-quantization-helpers-module">
          </span>
          <h2>
           sparseml.pytorch.utils.quantization.helpers module
           <a class="headerlink" href="#module-sparseml.pytorch.utils.quantization.helpers" title="Permalink to this headline">
            
           </a>
          </h2>
          <p>
           Helper functions for performing quantization aware training with PyTorch
          </p>
          <dl class="py class">
           <dt id="sparseml.pytorch.utils.quantization.helpers.QATWrapper">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              QATWrapper
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               forward_fn
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               num_inputs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               1
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               kwarg_input_names
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               num_outputs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               1
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               input_qconfigs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.quantization.qconfig.QConfig
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.quantization.qconfig.QConfig
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'asymmetric'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_qconfigs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.quantization.qconfig.QConfig
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.quantization.qconfig.QConfig
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'asymmetric'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#QATWrapper">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.QATWrapper" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </code>
            </p>
            <p>
             Wraps inputs and outputs of a Module or function with QuantStubs for
Quantization-Aware-Training (QAT)
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  forward_fn
                 </strong>
                 – function to be wrapped, should generally accept and return
torch Tensor(s)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  num_inputs
                 </strong>
                 – number of inputs of the forward function to add a QuantStub
to. Will wrap the first num_inputs ordered inputs of the function. Default
is 1
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  kwarg_input_names
                 </strong>
                 – list of names of key word arguments to the forward pass
that should be wrapped with a fake quantize operation. Defaults to empty
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  num_outputs
                 </strong>
                 – number of outputs of the forward function to add a QuantStub
to. Will wrap the first num_inputs ordered outputs of the function. Default
is 1. Will also add a DeQuantStub for FP32 conversion if
torch.quantization.convert is invoked
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  input_qconfigs
                 </strong>
                 – QConfig to use for calibrating the input QuantStubs. Can
be a single QConfig that will be copied to each QuantStub or a list of one
QConfig for each input. Instead of a QConfig objects, the string ‘asymmetric’
or ‘symmetric’ may be used to use default UINT8 asymmetric and symmetric
quantization respectively
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  output_qconfigs
                 </strong>
                 – QConfig to use for calibrating the output QuantStubs. Can
be a single QConfig that will be copied to each QuantStub or a list of one
QConfig for each output. Instead of a QConfig objects, the string ‘asymmetric’
or ‘symmetric’ may be used to use default UINT8 asymmetric and symmetric
quantization respectively
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt id="sparseml.pytorch.utils.quantization.helpers.QATWrapper.configure_qconfig">
              <code class="sig-name descname">
               <span class="pre">
                configure_qconfig
               </span>
              </code>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#QATWrapper.configure_qconfig">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.QATWrapper.configure_qconfig" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
              <p>
               Sets the qconfigs of the quant stubs to the pre-initialized QConfigs
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt id="sparseml.pytorch.utils.quantization.helpers.QATWrapper.forward">
              <code class="sig-name descname">
               <span class="pre">
                forward
               </span>
              </code>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 *
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 args
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               Any
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#QATWrapper.forward">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.QATWrapper.forward" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    args
                   </strong>
                   – arguments to forward function; the first num_inputs of these args
will be wrapped by a QuantStub
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – key word arguments to pass to the wrapped forward function
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 outputs of the forward function with a QuantStub applied to the first
num_outputs outputs
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt id="sparseml.pytorch.utils.quantization.helpers.QATWrapper.from_module">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <code class="sig-name descname">
               <span class="pre">
                from_module
               </span>
              </code>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <a class="reference internal" href="#sparseml.pytorch.utils.quantization.helpers.QATWrapper" title="sparseml.pytorch.utils.quantization.helpers.QATWrapper">
               <span class="pre">
                sparseml.pytorch.utils.quantization.helpers.QATWrapper
               </span>
              </a>
              <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#QATWrapper.from_module">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.QATWrapper.from_module" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  module
                 </strong>
                 – torch Module to create a QATWrapper for
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 QATWrapper object created using the given Module as the forward
function. Will attempt to find any other named parameter of the QATWrapper
constructor from the attributes of the given Module
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt id="sparseml.pytorch.utils.quantization.helpers.QATWrapper.training">
              <code class="sig-name descname">
               <span class="pre">
                training
               </span>
              </code>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.QATWrapper.training" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.helpers.add_quant_dequant">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              add_quant_dequant
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#add_quant_dequant">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.add_quant_dequant" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             Wraps all Conv and Linear submodule with a qconfig with a QuantWrapper
:param module: the module to modify
            </p>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.helpers.configure_module_default_qconfigs">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              configure_module_default_qconfigs
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#configure_module_default_qconfigs">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.configure_module_default_qconfigs" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             if any submodule of the given module has a configure_qconfig function,
configure_qconfig will be called on that submodule to set the qconfig(s) of that
module to its default
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                module
               </strong>
               – module to set qconfigs for
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.helpers.configure_module_qat_wrappers">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              configure_module_qat_wrappers
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#configure_module_qat_wrappers">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.configure_module_qat_wrappers" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             if any submodule of the given module has the attribute wrap_qat == True,
then it will be replaced by a QATWrapper of it created by QATWrapper.from_module.
Other named kwargs to the QATWrapper constructor must be contained in a dictionary
under an attributed named
             <cite>
              qat_wrapper_kwargs
             </cite>
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                module
               </strong>
               – module to potentially wrap the submodules of
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.helpers.fuse_module_conv_bn_relus">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              fuse_module_conv_bn_relus
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inplace
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               override_bn_subclasses_forward
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               bool
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             torch.nn.modules.module.Module
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#fuse_module_conv_bn_relus">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.fuse_module_conv_bn_relus" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             Performs fusion of Conv2d, BatchNorm2d, and ReLU layers found in the
given module. To be fused, these layers must appear sequentially in
module.named_modules() and be in the same submodule.
Fuses either Conv2d -&gt; BatchNorm2d or Conv2d -&gt; BatchNorm2d -&gt; ReLU blocks
            </p>
            <p>
             If this function does not fuse the model in the desired way, implement an
in place fusing function for the model.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module to fuse
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inplace
                 </strong>
                 – set True to perform fusions in-place. default is True
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  override_bn_subclasses_forward
                 </strong>
                 – if True, modules that are subclasses of
BatchNorm2d will be modified to be BatchNorm2d but with the forward
pass and state variables copied from the subclass. This is so these
BN modules can pass PyTorch type checking when fusing. Can set to
“override-only” and only parameters will be overwritten, not the
forward pass. Default is True
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               the fused module
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.helpers.get_qat_qconfig">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              get_qat_qconfig
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               symmetric_activations
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               symmetric_weights
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             torch.quantization.qconfig.QConfig
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#get_qat_qconfig">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.get_qat_qconfig" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  symmetric_activations
                 </strong>
                 – if True, activations will have a symmetric
UINT8 quantization range with zero point set to 128. Otherwise activations
will use asymmetric quantization with any zero point. Default is False
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  symmetric_weights
                 </strong>
                 – if True, weights will have a symmetric
INT8 quantization range with zero point set to 0. Otherwise activations
will use asymmetric quantization with any zero point. Default is True
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               A QAT fake quantization config for symmetric weight quantization and
asymmetric activation quantization.  The difference between this and
torch.quantization.default_qat_qconfig is that the activation observer
will not have reduce_range enabled.
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.helpers.prepare_embeddings_qat">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              prepare_embeddings_qat
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               qconfig
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.quantization.qconfig.QConfig
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#prepare_embeddings_qat">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.prepare_embeddings_qat" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             adds a fake quantize call to the weights of any Embedding modules in the given
module
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – module to run QAT for the embeddings of
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  qconfig
                 </strong>
                 – qconfig to generate the fake quantize ops from. Default uses INT8
asymmetric range
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.utils.quantization.quantize_qat_export">
          <span id="sparseml-pytorch-utils-quantization-quantize-qat-export-module">
          </span>
          <h2>
           sparseml.pytorch.utils.quantization.quantize_qat_export module
           <a class="headerlink" href="#module-sparseml.pytorch.utils.quantization.quantize_qat_export" title="Permalink to this headline">
            
           </a>
          </h2>
          <p>
           Helper functions for parsing an exported pytorch model trained with
quantization aware training.
          </p>
          <dl class="py class">
           <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              QuantizationParams
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               scale
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               zero_point
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               target
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               tuple
              </span>
             </code>
            </p>
            <dl class="py method">
             <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.scale">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <code class="sig-name descname">
               <span class="pre">
                scale
               </span>
              </code>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.scale" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
              <p>
               Alias for field number 0
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.target">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <code class="sig-name descname">
               <span class="pre">
                target
               </span>
              </code>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.target" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
              <p>
               Alias for field number 2
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.zero_point">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <code class="sig-name descname">
               <span class="pre">
                zero_point
               </span>
              </code>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.zero_point" title="Permalink to this definition">
               
              </a>
             </dt>
             <dd>
              <p>
               Alias for field number 1
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.get_quantization_params">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              get_quantization_params
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               onnx.onnx_ml_pb2.ModelProto
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               node
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               onnx.onnx_ml_pb2.NodeProto
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               include_target
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <a class="reference internal" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams" title="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams
             </span>
            </a>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/quantize_qat_export.html#get_quantization_params">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.get_quantization_params" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  model
                 </strong>
                 – ONNX model to read from
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  node
                 </strong>
                 – A QuantizeLinear or DequantizeLinear Node
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  include_target
                 </strong>
                 – Set True include quantization target. If False,
target value will be returned as None. Default is None
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               QuantizationParams object with scale and zero point, will include the
quantization target if it is an initializer otherwise target will be None
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.quantize_torch_qat_export">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              quantize_torch_qat_export
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               onnx.onnx_ml_pb2.ModelProto
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_file_path
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inplace
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             onnx.onnx_ml_pb2.ModelProto
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/quantize_qat_export.html#quantize_torch_qat_export">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.quantize_torch_qat_export" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  model
                 </strong>
                 – The model to convert, or a file path to it
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  output_file_path
                 </strong>
                 – File path to save the converted model to
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inplace
                 </strong>
                 – If true, does conversion of model in place. Default is true
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               Converts a model exported from a torch QAT session from a QAT graph with
fake quantize ops surrounding operations to a quantized graph with quantized
operations. All quantized Convs and FC inputs and outputs be surrounded by
fake quantize ops
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt id="sparseml.pytorch.utils.quantization.quantize_qat_export.skip_onnx_input_quantize">
            <code class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </code>
            <code class="sig-name descname">
             <span class="pre">
              skip_onnx_input_quantize
             </span>
            </code>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               onnx.onnx_ml_pb2.ModelProto
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_file_path
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/quantize_qat_export.html#skip_onnx_input_quantize">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.skip_onnx_input_quantize" title="Permalink to this definition">
             
            </a>
           </dt>
           <dd>
            <p>
             If the given model has a single FP32 input that feeds into a QuantizeLinear
node, then the input will be changed to uint8 and the QuantizeLinear node will be
deleted. This enables quantize graphs to take quantized inputs instead of floats.
            </p>
            <p>
             If no optimization is made, a RuntimeError will be raised.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  model
                 </strong>
                 – The model to convert, or a file path to it
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  output_file_path
                 </strong>
                 – File path to save the converted model to
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.utils.quantization">
          <span id="module-contents">
          </span>
          <h2>
           Module contents
           <a class="headerlink" href="#module-sparseml.pytorch.utils.quantization" title="Permalink to this headline">
            
           </a>
          </h2>
          <p>
           Tools for quantizing and exporting PyTorch models
          </p>
         </div>
        </div>
       </div>
      </div>
      <footer>
       <div aria-label="Footer" class="rst-footer-buttons" role="navigation">
        <a accesskey="p" class="btn btn-neutral float-left" href="sparseml.pytorch.utils.html" rel="prev" title="sparseml.pytorch.utils package">
         <span aria-hidden="true" class="fa fa-arrow-circle-left">
         </span>
         Previous
        </a>
        <a accesskey="n" class="btn btn-neutral float-right" href="sparseml.sparsification.html" rel="next" title="sparseml.sparsification package">
         Next
         <span aria-hidden="true" class="fa fa-arrow-circle-right">
         </span>
        </a>
       </div>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the "License").
        </p>
       </div>
       Built with
       <a href="https://www.sphinx-doc.org/">
        Sphinx
       </a>
       using a
       <a href="https://github.com/readthedocs/sphinx_rtd_theme">
        theme
       </a>
       provided by
       <a href="https://readthedocs.org">
        Read the Docs
       </a>
       .
      </footer>
     </div>
    </div>
   </section>
  </div>
  <div aria-label="versions" class="rst-versions" data-toggle="rst-versions" role="note">
   <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book">
     Other Versions
    </span>
    v: v0.7.0
    <span class="fa fa-caret-down">
    </span>
   </span>
   <div class="rst-other-versions">
    <dl>
     <dt>
      Tags
     </dt>
     <dd>
      <a href="../v0.3.0/api/sparseml.pytorch.utils.quantization.html">
       v0.3.0
      </a>
     </dd>
     <dd>
      <a href="../v0.3.1/api/sparseml.pytorch.utils.quantization.html">
       v0.3.1
      </a>
     </dd>
     <dd>
      <a href="../v0.4.0/api/sparseml.pytorch.utils.quantization.html">
       v0.4.0
      </a>
     </dd>
     <dd>
      <a href="../v0.5.0/api/sparseml.pytorch.utils.quantization.html">
       v0.5.0
      </a>
     </dd>
     <dd>
      <a href="../v0.5.1/api/sparseml.pytorch.utils.quantization.html">
       v0.5.1
      </a>
     </dd>
     <dd>
      <a href="../v0.6.0/api/sparseml.pytorch.utils.quantization.html">
       v0.6.0
      </a>
     </dd>
     <dd>
      <a href="sparseml.pytorch.utils.quantization.html">
       v0.7.0
      </a>
     </dd>
    </dl>
    <dl>
     <dt>
      Branches
     </dt>
     <dd>
      <a href="../main/api/sparseml.pytorch.utils.quantization.html">
       main
      </a>
     </dd>
    </dl>
   </div>
  </div>
  <script>
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <!-- Theme Analytics -->
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-128364174-1">
  </script>
  <script>
   window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-128364174-1', {
          'anonymize_ip': false,
      });
  </script>
 </body>
</html>